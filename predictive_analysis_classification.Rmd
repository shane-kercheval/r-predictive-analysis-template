---
title: "Predictive Analysis"
author: "Shane Kercheval"
output:
  md_document:
    variant: markdown_github
    toc: true
    toc_depth: 3
---

```{r setup, echo = FALSE, results='hide', message=FALSE, warning=FALSE}
options(scipen=999) # non-scientific notation
options(width=180)
source('~/r-tools/output/plots.R', chdir=TRUE)
source('~/r-tools/output/markdown.R', chdir=TRUE)
source('~/r-tools/general/model_measurements.R', chdir=TRUE)
library('data.table')
library(partykit) # Use the partykit package to make some nice plots. First, convert the rpart objects to party objects.
library(psych)
library(knitr)
library(scales)
library(DMwR)
library(e1071)
library(Hmisc)
library(corrplot)
library(stringr)
library(tidyverse)
library(reshape2)
library(caret)
library(gmodels)
library(randomForest)
library(ROCR)
library(pls)
library(glmnet)
library(Matrix)
library(sparseLDA)
library(klaR)
library(mda)
library(class)
library(MASS)

opts_chunk$set(out.width='750px', dpi=200)

require(doMC)
registerDoMC(cores = round(detectCores()/2) + 1) # All subsequent models are then run in parallel - https://topepo.github.io/caret/parallel-processing.html

calculate_nnet_MaxNWts <- function(max_nnet_size, number_of_dataset_columns) { # number_of_dataset_columns is all columns including target
	return ((max_nnet_size * number_of_dataset_columns) + max_nnet_size + 1) # from pg 361, but slightly different because their `reducedSet` (vector rather than df) does not include the target variable, so ignoring the `+1` they have
}

custom_seed <- 123
refresh_models <- FALSE
```

```{r dataset_definition, echo = FALSE}
#######################################################################################################################################
# define the dataset:
# the code below expects a dataset with an dependent variable named `target`, who's value is a factor with defined classes below
# also, all logical and character columns should be converted to factors
#######################################################################################################################################
df_credit <- fread('./classification_data/credit.csv', data.table = FALSE)
classification_dataset <- df_credit %>%
	dplyr::rename(target = default) %>%
	dplyr::mutate(phone = ifelse(phone == 'yes', TRUE, FALSE)) %>% # technically don't need to do this but there seems to be an error in dplyr.mutate_if https://github.com/tidyverse/dplyr/pull/2011 which says it is resolved, but still seems to be a problem, dispite having latest version
	dplyr::select(target, everything())
classification_dataset <- as.data.frame(classification_dataset)

target_positive_class <- 'yes'
target_negative_class <- 'no'
```

```{r dataset_prepare, echo = FALSE}
classification_dataset$target <- factor(classification_dataset$target, levels = c(target_positive_class, target_negative_class))

classification_dataset <- classification_dataset %>%
	dplyr::mutate_if(is.character, as.factor) %>%
	dplyr::mutate_if(is.logical, as.factor)

classification_column_names <- colnames(classification_dataset)
numeric_columns <- map_lgl(classification_column_names, ~ {
	return (is.numeric(classification_dataset[, .]))
})
numeric_column_names <- classification_column_names[numeric_columns]

num_sample_size <- nrow(classification_dataset)
num_predictors <- length(classification_dataset) - 1
```

# Tuning Parameters

```{r tuning_parameters}
# train/test set
training_percentage <- 0.90

# cross validation
cross_validation_num_folds <- 10
cross_validation_num_repeats <- 3

# tuning parameters
tuning_number_of_latent_variables_to_retain <- 1:10

tuning_glmnet_alpha <- seq(from = 0, to = 1, length = 5) # alpha = 0 is pure ridge regression, and alpha = 1 is pure lasso regression.
tuning_glmnet_lambda <- seq(from = 0.0001, to = 1, length = 50) # lambda values control the amount of penalization in the model.

tuning_nearest_shrunken_centroids_shrinkage_threshold <- data.frame(threshold = 0:25)

tuning_mda_subclasses <- 1:8

tuning_rda_lambda <- seq(from = 0, to = 1, by = 0.2)
tuning_rda_gamma <- seq(from = 0, to = 1, by = 0.2)

tuning_nnet_size <- 1:10
tuning_nnet_decay <- c(0, 0.1, 1, 2)
parameter_nnet_linout <- FALSE
parameter_nnet_max_iterations <- 2000

tuning_svm_linear_num_costs <- 5
tuning_svm_poly_num_costs <- 3
tuning_svm_radial_num_costs <- 6
```

# Dataset

> Assumes the dataset has factors for strings; logical for TRUE/FALSE; `target` for outcome variable

## Summary

> Total predictors: ``r num_predictors``

> Total data-points/rows: ``r num_sample_size``

> Number of training data-points: ``r round(num_sample_size * training_percentage)``

Rule of thumbs for dimensions (Probabilistic and Statistical Modeling in Computer Science; pg 430):

> r < sqrt(n); where r is the number of predictors and sqrt(n) is the square root of the sample size (``r round(sqrt(num_sample_size))``): ``r num_predictors < round(sqrt(num_sample_size))``

> r < sqrt(n_t); where r is the number of predictors and sqrt(n_t) is the square root of the training set size (``r round(sqrt(round(num_sample_size * training_percentage)))``): ``r num_predictors < round(sqrt(round(num_sample_size * training_percentage)))``

```{r summary, echo = FALSE}
summary(classification_dataset)
```

## Skewness

Note: `Box-Cox` can only be applied to sets (i.e. predictors) where all values are `> 0`. So some/most/all? `NA`s will be from that limiation.

```{r skewness, echo = FALSE}
skewnewss_statistics <- map_dbl(classification_column_names, ~ {
	column_name <- .

	dataset_column <- classification_dataset[, column_name]
	if(is.numeric(dataset_column) && min(dataset_column > 0))
	{
		return (BoxCoxTrans(dataset_column)$skewness)
	}else{
		return (NA)
	}
})

kable(data.frame(column = classification_column_names, boxcox_skewness = skewnewss_statistics))
```

## Outliers

```{r outliers, echo = FALSE}
df_outliers <- map(numeric_column_names, ~ {
	numeric_data <- classification_dataset[, .]

	outlier_range <- 2 # this multipied by IQR (traditionally this number is 1.5)

	lower_quantile <- quantile(numeric_data)[2]
	upper_quantile <- quantile(numeric_data)[4]
	iqr <- upper_quantile - lower_quantile

	threshold_upper <- (iqr * outlier_range) + upper_quantile
	threshold_lower <- lower_quantile - (iqr * 1.5) # Any numeric_data point outside (> threshold_upper or < threshold_lower) these values is a mild outlier

	upper_outlier_count <- sum(numeric_data > threshold_upper)
	lower_outlier_count <- sum(numeric_data < threshold_lower)

	return (list(upper_outlier_count, lower_outlier_count))
})

df_outliers <- data.frame(columns = numeric_column_names, lower_outlier_count = map_chr(df_outliers, ~ {.[[2]]}), upper_outlier_count = map_chr(df_outliers, ~ {.[[1]]}))
kable(df_outliers)
```

## Correlation & Collinearity

### Correlation

```{r correlation, echo = FALSE, fig.height=5, fig.width=5}
if(sum(numeric_columns == TRUE) >= 2) {

	#pairs.panels(classification_dataset[, numeric_columns])
	matrix_correlations <- cor(classification_dataset[, numeric_columns])
	corrplot::corrplot(matrix_correlations, order = 'hclust', tl.cex = .35, col = colorRampPalette(rev(c('#67001F', '#B2182B', '#D6604D', '#F4A582', '#FDDBC7', '#FFFFFF', '#D1E5F0', '#92C5DE', '#4393C3', '#2166AC', '#053061')))(200))
} else {
	cat('\n\n> Not enough numberic columns for correlation.\n\n')
}
```

### Collinearity Removal

```{r removal_setup, echo = FALSE}
correlation_threshold <- 0.90
```

#### Caret's `findCorrelation`

Shows caret's recommendation of removing collinear columns based on correlation threshold of ``r correlation_threshold``

```{r caret_collinearity, echo = FALSE}
numeric_predictor_data <- classification_dataset[, numeric_columns]
collinear_indexes <- findCorrelation(cor(numeric_predictor_data), correlation_threshold)
if(length(collinear_indexes) == 0)
{
	recommended_columns_caret <- colnames(classification_dataset)
}else{
	recommended_columns_caret <- c('target', colnames(numeric_predictor_data)[-collinear_indexes])	
}
```

> columns recommended for removal: ``r paste(colnames(numeric_predictor_data)[collinear_indexes], collapse = ', ')``

> final columns recommended: ``r paste(recommended_columns_caret, collapse = ', ')``

#### Heuristic

This method is described in APM pg 47 as the following steps

- calculate the correlation matrix of predictors
- determine the two predictors associated with the largest absolute pairwise correlation (call them predictors `A` and `B`)
- Determine the average correlation between `A` and the other variables.
	- Do the same for `B`
- If `A` has a larger average correlation, remove it; otherwise, remove predcitor `B`
- Repeat until no absolute correlations are above the threshold (``r correlation_threshold``)

```{r heuristic_collinearity, echo = FALSE}
if(sum(numeric_columns == TRUE) >= 2) {

	predictor_correlations <- cor(numeric_predictor_data)

	walk(1:nrow(predictor_correlations), ~ {
		predictor_correlations[.,.] <<- NA	
	})

	while(max(abs(predictor_correlations), na.rm = TRUE) > correlation_threshold) {
		## caret's findCorrelation function is used to identify columns to remove.
		num_cols_rows <- nrow(predictor_correlations)
		#highCorr <- findCorrelation(predictor_correlations, .75)
		ab_index <- which(abs(predictor_correlations) == max(abs(predictor_correlations), na.rm = TRUE) & abs(predictor_correlations) > 0.75)[1] # get the first index of the highest correlation
		a_index <- ceil(ab_index / num_cols_rows) # get the row number 
		b_index <- ab_index - (num_cols_rows * (a_index-1))
	
		ave_a_predictor_correlations <- mean(abs(predictor_correlations[a_index, ]), na.rm = TRUE)
		ave_b_predictor_correlations <- mean(abs(predictor_correlations[b_index, ]), na.rm = TRUE)
	
		if(ave_a_predictor_correlations > ave_b_predictor_correlations)
		{
			predictor_correlations <<- predictor_correlations[-a_index, -a_index]

		}else {

			predictor_correlations <<- predictor_correlations[-b_index, -b_index]
		}	
		
	}
	recommended_numeric_columns <- colnames(predictor_correlations)
	non_numeric_columns <- classification_column_names[!(classification_column_names %in% numeric_column_names)] # includes target
	recommended_columns_custom <- c(recommended_numeric_columns, non_numeric_columns)
	recommended_removal <- classification_column_names[!(classification_column_names %in% recommended_columns_custom)]
	
} else {

	cat('\n\n> Not enough numberic columns for collinearity. Not removing any columns.\n\n')
	recommended_columns_custom <- classification_column_names
	recommended_removal <- ''
}
```

> columns recommended for removal: ``r paste(recommended_removal, collapse = ', ')``

> final columns recommended: ``r paste(recommended_columns_custom, collapse = ', ')``

## Graphs

```{r graphs, echo = FALSE, fig.height=4, fig.width=6, comment = '', results = 'asis'}
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

colors_two = rev(gg_color_hue(2))

walk(classification_column_names, ~ {
	column_name <- .
	
	if(column_name == 'target')
	{
		return ()
	}

	cat(paste('\n\n###', column_name, '\n\n'))
	dataset_column <- classification_dataset[, column_name]
	if(is.numeric(dataset_column))
	{
		df_data <- data.frame(target=dataset_column)
		fun_args <- list(mean = mean(df_data$target), sd = sd(df_data$target))
		quantiles <- quantile(df_data$target, c(0.02, 0.98)) # cut off any 'outliers' (crude) so it doesn't skew graph
		
		result = tryCatch({
			print(ggplot(data = df_data, mapping = aes(x = target)) +
				geom_histogram(mapping=aes(y = ..density..), bins = 30) + # histogram of data
				geom_density(col = 'red') + # emperical PDF
				stat_function(fun = dnorm, args = fun_args, col = 'blue') + # theoretical normal PDF, based on data's mean & standard deviation
				coord_cartesian(xlim = c(quantiles[1], quantiles[2])) + labs(x = column_name))

		}, error = function(e) {
			cat('\n\nError with histogram.\n\n')
		})

		print(ggplot(data = df_data, mapping = aes(x = column_name,  y = target, group = 1)) +
				geom_boxplot() +
				xlab(column_name) + ylab(column_name))

		print(ggplot(data = classification_dataset, mapping = aes(x = target, y = dataset_column)) +
				geom_boxplot() + 
				xlab('target') + ylab(column_name))
		
		if(length(levels(classification_dataset$target)) == 2){
			levels <- levels(classification_dataset$target)
			level_1 <- levels[1]
			level_2 <- levels[2]

			level_1_values <- (classification_dataset %>% filter(target == level_1))[, column_name]
			level_2_values <- (classification_dataset %>% filter(target == level_2))[, column_name]

			#https://www.r-bloggers.com/two-sample-students-t-test-1/
			# Before proceeding with the t-test, it is necessary to evaluate the sample variances of the two groups, using a Fisher’s F-test to verify the homoskedasticity (homogeneity of variances).
			homogeneous <- var.test(level_1_values, level_2_values)$p.value > 0.05 # If we obtain a p-value greater than 0.05, then we fail to reject the null and can assume that the two variances are homogeneous
			different_means <- t.test(level_1_values, level_2_values, var.equal = homogeneous, paired = FALSE)$p.value <= 0.05 # if we obtain a p-value of <= 0.05 then we reject the null and conclude that the true difference in means is not equal to 0
			
			cat(paste('\n\nstatistically different means (check assumptions for t-test):', different_means, '\n\n'))

			# https://www.r-bloggers.com/wilcoxon-mann-whitney-rank-sum-test-or-test-u/
			# Comparison of the averages of two independent groups of samples, of which we can not assume a distribution of Gaussian type; is also known as Mann-Whitney U-test.
			cat('\n\nThe Wilcoxon-Matt-Whitney test (or Wilcoxon rank sum test, or Mann-Whitney U-test) is used when is asked to compare the means of two groups that do not follow a normal distribution: it is a non-parametrical test. (https://www.r-bloggers.com/wilcoxon-mann-whitney-rank-sum-test-or-test-u/)')

			different_means <- wilcox.test(level_1_values, level_2_values, correct = FALSE, paired = FALSE)$p.value <= 0.05
			cat(paste('\n\nstatistically different means (Wilcoxon-Matt-Whitney):', different_means, '\n\n'))
		}

	}else if(is.factor(dataset_column) || is.logical(dataset_column)){
		
		print(ggplot(data = classification_dataset, mapping = aes(x = dataset_column)) + 
			geom_bar(mapping=aes(fill = dataset_column)) + 
			ggtitle(paste('Histogram of', column_name)) + ylab('Count') + xlab(column_name) + theme(axis.text.x = element_text(angle = 50, hjust = 1)) + guides(fill = FALSE))

		df_data <- data.frame(categorical_variable=classification_dataset$target, column = classification_dataset[, column_name ])
		print(ggplot(df_data, aes(column, ..count..)) +
				geom_bar(aes(fill = categorical_variable), position = 'dodge') +
				ggtitle(paste('Histogram of', column_name, ', grouped by the target variable')) + ylab('Count') + xlab(column_name) + theme(axis.text.x = element_text(angle = 50, hjust = 1)) + scale_fill_manual(values=colors_two))

		result = tryCatch({
			count_table <- table(classification_dataset[, c('target', column_name)])
			cat(paste0('\n\n> Chi-Square p-value: `', round(chisq.test(count_table)$p.value, 3), '`\n\n'))
		}, error = function(e) {
			cat(paste('\n\n', e, '\n\n'))
		})

	}else{
		stop(paste('unknown data type -', column_name))
	}
})
```

# Spot-Check

## Configuration

```{r spot_check_prepare_classification, echo = FALSE}
set.seed(custom_seed)
training_index <- createDataPartition(classification_dataset$target, p = training_percentage, list = TRUE)
classification_train <- classification_dataset[training_index$Resample1, ]
classification_test <- classification_dataset[-training_index$Resample1, ]
```

### Class Balance 

Make sure class balance is even amount training/test datasets.

### Training Data

```{r echo = FALSE}
prop.table(table(classification_train$target))
```

### Test

```{r echo = FALSE}
prop.table(table(classification_test$target))
```

```{r spot_check_classification, echo = FALSE}
# we have to make sure they are fit on exactly the same test sets in training and cross validation so we are sure we are making an apples-to-apples comparison
set.seed(custom_seed)
spot_check_folds <- createMultiFolds(classification_train$target, k = cross_validation_num_folds, times = cross_validation_num_repeats) # maintains the `churn` ratio

train_control <- trainControl(
	summaryFunction = twoClassSummary,
	classProbs = TRUE,
	method = 'repeatedcv',
	number = cross_validation_num_folds,
	repeats = cross_validation_num_repeats,
	verboseIter = FALSE,
	savePredictions = TRUE,
	index = spot_check_folds)

metric <- 'ROC'
classification_models <- list()
model_preprocessing_values <- data.frame(model = NULL, training_preProcess = NULL)
```

> Using ``r cross_validation_num_folds``-fold cross-validation with ``r cross_validation_num_repeats`` repeats, using the ``r metric`` statistic to evaluate each model.

> used ``r percent(training_percentage)`` of data for `training` set (``r nrow(classification_train)``), and ``r percent(1 - training_percentage)`` for `test` set (``r nrow(classification_test)``).

### Testing train_classification

> NOTE that for logistic regression (GLM), caret's `train()` (because of `glm()`) uses the second-level factor value as the success/postive event but `resamples()` uses the first-level as the success event. The result is either the `sensitivity` and `specificity` for `resamples()` will be reversed (and so I would be unable to compare apples to apples with other models), or I need to keep the first-level factor as the positive event (the default approach), which will mean that THE COEFFICIENTS WILL BE REVERSED, MAKIN THE MODEL RELATIVE TO THE NEGATIVE EVENT. I chose the latter, in order to compare models below, but this means that when using the logistic model to explain the data, the reader needs to mentally reverse the direction/sign of the coefficients, or correct the problem in the final stages of model building.

> NOTE: "Logistic regression does not make many of the key assumptions of linear regression and general linear models that are based on ordinary least squares algorithms – particularly regarding linearity, normality, homoscedasticity, and measurement level." [link](http://www.statisticssolutions.com/assumptions-of-logistic-regression/)

```{r run_spot_check, echo = FALSE, comment = '', results = 'asis'}
train_classification <- function(	model_name,
									dataset,
									custom_seed,
									training_method,
									training_metric,
									training_preProcess,
									training_control,
									training_tune_grid = NULL,
									training_tune_length = NULL,
									refresh_models = TRUE,
									sample_to_predictor_ratio_threshold = NULL,
									class_to_predictor_ratio_threshold = NULL,
									class_names = NULL,
									neural_net_numWts = NULL,
									neural_net_lineout = NULL,
									neural_net_max_iterations = NULL) {
	caret_model <- NULL
	
	if(!is.null(training_tune_grid)) {

		model_tuning_parameters <- as.character(modelLookup(training_method)$parameter)
		model_tuning_parameters <- model_tuning_parameters[model_tuning_parameters != 'parameter']
		stopifnot(colnames(training_tune_grid) %in% model_tuning_parameters) # make sure all the parameters passed in exist in the tuning parameters
		stopifnot(model_tuning_parameters %in% colnames(training_tune_grid)) # make sure all the tuning parameters exist in the grid
	}

	if(refresh_models) {

		check_data(	dataset = dataset,
					sample_to_predictor_ratio_threshold = sample_to_predictor_ratio_threshold,
					class_to_predictor_ratio_threshold = class_to_predictor_ratio_threshold,
					class_names = class_names)

		set.seed(custom_seed)
		if(!is.null(neural_net_numWts) || !is.null(neural_net_max_iterations) || !is.null(neural_net_lineout)) { # neural network
			
			stopifnot(!is.null(neural_net_numWts) && !is.null(neural_net_max_iterations) && !is.null(neural_net_lineout)) # if one of these parameters is not null, then non should be null
			cat(paste0('\n\n> Setting `nnet` specific parameters: `MaxNWts` - `', neural_net_numWts,'`; `maxit` - `', neural_net_max_iterations,'`\n\n'))
			caret_model <- train(	target ~ .,
									data = dataset,
									method = training_method,
									metric = training_metric,
									preProc = training_preProcess,
									trControl = training_control,
									tuneGrid = training_tune_grid,
				  					tuneLength = training_tune_length,
									trace = FALSE,
				  					maxit = neural_net_max_iterations,
				  					MaxNWts = neural_net_numWts,
				  					linout = neural_net_lineout)			
		} else { # non-neural network

			caret_model <- train(	target ~ .,
									data = dataset,
									method = training_method,
									metric = training_metric,
									preProc = training_preProcess,
									trControl = training_control,
									tuneGrid = training_tune_grid,
				  					tuneLength = training_tune_length)
		}

		saveRDS(caret_model, file = paste0('./classification_data/', model_name,'.RDS'))

	} else{

		caret_model <- readRDS(paste0('./classification_data/', model_name,'.RDS'))
	}

	summary_caret_model <- NULL
	expected_error = tryCatch(
		summary_caret_model <- summary(caret_model),# this causes an error with some models
		error=function(e) e)
	
	if(!is.null(summary_caret_model)) {
		
		cat('\n\n#### Model Summary\n\n')
		cat(codebc(summary_caret_model, postfix='\n\n'))
	}
	
	cat('\n\n#### Model Predictors\n\n')
	cat(codebc(predictors(caret_model), postfix='\n\n'))
	
	if(!is.null(training_tune_grid) || !is.null(training_tune_length)) { # if there are tuning parameters, we can plot them.
		
		cat('\n\n#### Model Tuning Grid Performance\n\n')
		print(plot(caret_model, top = 20, scales = list(y = list(cex = 0.95))))
		cat('\n\n')
	}
	
	cat('\n\n#### Variable Importance\n\n')
	cat(codebc(varImp(caret_model, scale = FALSE), postfix='\n\n'))
	
	return (caret_model)
}

#############################
## TEST train_classification
#############################
test_model_glm <- train_classification(	model_name = 'test',
						dataset = classification_train,
						custom_seed = custom_seed,
						training_method = 'glm',
						training_metric = metric,
						training_preProcess = c('nzv', 'center', 'scale'),
						training_control = train_control,
						training_tune_grid = NULL,
						training_tune_length = NULL,
						refresh_models = TRUE,
						sample_to_predictor_ratio_threshold = NULL,
						class_to_predictor_ratio_threshold = NULL,
						class_names = NULL)

stopifnot(file.exists('./classification_data/test.RDS'))
# expected_model_glm <- test_model_glm
# saveRDS(expected_model_glm, file = './classification_data/expected_model_glm.RDS')
expected_model_glm <- readRDS(file = './classification_data/expected_model_glm.RDS')

stopifnot(varImp(expected_model_glm)$importance$Overall == varImp(test_model_glm)$importance$Overall)
stopifnot(expected_model_glm$resample == test_model_glm$resample)

expected_error = tryCatch(
	train_classification(	model_name = 'test',
							dataset = classification_train,
							custom_seed = custom_seed,
							training_method = 'rda',
							training_metric = metric,
							training_preProcess = c('nzv', 'center', 'scale'),
							training_control = train_control,
							training_tune_grid = expand.grid(	#lambda = tuning_rda_lambda,
																gamma = tuning_rda_gamma),
							training_tune_length = NULL,
							refresh_models = TRUE,
							sample_to_predictor_ratio_threshold = NULL,
							class_to_predictor_ratio_threshold = NULL,
							class_names = NULL),
	error=function(e) e)
stopifnot(inherits(expected_error, 'error'))
stopifnot(expected_error$message == 'model_tuning_parameters %in% colnames(training_tune_grid) are not all TRUE')

expected_error = tryCatch(
	train_classification(	model_name = 'test',
							dataset = classification_train,
							custom_seed = custom_seed,
							training_method = 'rda',
							training_metric = metric,
							training_preProcess = c('nzv', 'center', 'scale'),
							training_control = train_control,
							training_tune_grid = expand.grid(	lambda = tuning_rda_lambda),
																#gamma = tuning_rda_gamma),
							training_tune_length = NULL,
							refresh_models = TRUE,
							sample_to_predictor_ratio_threshold = NULL,
							class_to_predictor_ratio_threshold = NULL,
							class_names = NULL),
	error=function(e) e)
stopifnot(inherits(expected_error, 'error'))
stopifnot(expected_error$message == 'model_tuning_parameters %in% colnames(training_tune_grid) are not all TRUE')
```

## Models

### glm_no_pre_process

```{r glm_no_pre_process, echo = FALSE, comment = '', results = 'asis'}
glm_no_pre_process <- train_classification(
	refresh_models = refresh_models,
	training_metric = metric,
	custom_seed = custom_seed,
	model_name = 'glm_no_pre_process',
	dataset = classification_train,
	training_method = 'glm',
	training_preProcess = NULL,
	training_control = train_control,
	training_tune_grid = NULL,
	training_tune_length = NULL,
	sample_to_predictor_ratio_threshold = 5,
	class_to_predictor_ratio_threshold = NULL,
	class_names = NULL)

model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'glm_no_pre_process', training_preProcess = I(list(NA))))
classification_models[['glm_no_pre_process']] <- glm_no_pre_process
```

### glm_basic_processing

```{r glm_basic_processing, echo = FALSE, comment = '', results = 'asis'}
model_pre_processing <- c('nzv', 'center', 'scale')
glm_basic_processing <- train_classification(
	refresh_models = refresh_models,
	training_metric = metric,
	custom_seed = custom_seed,
	model_name = 'glm_basic_processing',
	dataset = classification_train,
	training_method = 'glm',
	training_preProcess = model_pre_processing,
	training_control = train_control,
	training_tune_grid = NULL,
	training_tune_length = NULL,
	sample_to_predictor_ratio_threshold = 5,
	class_to_predictor_ratio_threshold = NULL,
	class_names = NULL)

model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'glm_basic_processing', training_preProcess = I(list(model_pre_processing))))
classification_models[['glm_basic_processing']] <- glm_basic_processing
```

### glm_remove_collinearity_caret

```{r glm_remove_collinearity_caret, echo = FALSE, comment = '', results = 'asis'}
if(all(classification_column_names %in% recommended_columns_caret)) { # if all the original column names exist in the recommended column names (i.e. columns are the same;  there are no collinear columns) then these results won't differ, we can skip.

	cat('\n\n> No collinear columns removed... skipping.\n\n')
} else {

	model_pre_processing <- c('nzv', 'center', 'scale')
	glm_remove_collinearity_caret <- train_classification(
		refresh_models = refresh_models,
		training_metric = metric,
		custom_seed = custom_seed,
		model_name = 'glm_remove_collinearity_caret',
		dataset = classification_train[, recommended_columns_caret],
		training_method = 'glm',
		training_preProcess = model_pre_processing,
		training_control = train_control,
		training_tune_grid = NULL,
		training_tune_length = NULL,
		sample_to_predictor_ratio_threshold = 5,
		class_to_predictor_ratio_threshold = NULL,
		class_names = NULL)

	model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'glm_remove_collinearity_caret', training_preProcess = I(list(model_pre_processing))))
	classification_models[['glm_remove_collinearity_caret']] <- glm_remove_collinearity_caret
}
``` 

### glm_remove_collinearity_custom

```{r glm_remove_collinearity_custom, echo = FALSE, comment = '', results = 'asis'}
if(all(classification_column_names %in% recommended_columns_custom)) { # if all the original column names exist in the recommended column names (i.e. columns are the same;  there are no collinear columns) then these results won't differ, we can skip.

	cat('\n\n> No collinear columns removed... skipping.\n\n')
} else {

	model_pre_processing <- c('nzv', 'center', 'scale')
	glm_remove_collinearity_custom <- train_classification(
		refresh_models = refresh_models,
		training_metric = metric,
		custom_seed = custom_seed,
		model_name = 'glm_remove_collinearity_custom',
		dataset = classification_train[, recommended_columns_custom],
		training_method = 'glm',
		training_preProcess = model_pre_processing,
		training_control = train_control,
		training_tune_grid = NULL,
		training_tune_length = NULL,
		sample_to_predictor_ratio_threshold = 5,
		class_to_predictor_ratio_threshold = NULL,
		class_names = NULL)

	model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'glm_remove_collinearity_custom', training_preProcess = I(list(model_pre_processing))))
	classification_models[['glm_remove_collinearity_custom']] <- glm_remove_collinearity_custom
}
```

### glm_yeojohnson

```{r glm_yeojohnson, echo = FALSE, comment = '', results = 'asis'}
model_pre_processing <- c('nzv', 'YeoJohnson', 'center', 'scale')
glm_yeojohnson <- train_classification(
	refresh_models = refresh_models,
	training_metric = metric,
	custom_seed = custom_seed,
	model_name = 'glm_yeojohnson',
	dataset = classification_train,
	training_method = 'glm',
	training_preProcess = model_pre_processing,
	training_control = train_control,
	training_tune_grid = NULL,
	training_tune_length = NULL,
	sample_to_predictor_ratio_threshold = 5,
	class_to_predictor_ratio_threshold = NULL,
	class_names = NULL)

model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'glm_yeojohnson', training_preProcess = I(list(model_pre_processing))))
classification_models[['glm_yeojohnson']] <- glm_yeojohnson
```

### logistic_regression_stepwise_backward

```{r logistic_regression_stepwise_backward, echo = FALSE, comment = '', results = 'asis'}
stepwise_logistic_columns <- NULL

if(refresh_models) {
	
	if(sum(numeric_columns == TRUE) >= 1) { # have to have at least 1 numeric column to pre-process
		
		set.seed(custom_seed)
		pre_processed_numeric_data <- preProcess(classification_train, method = c('nzv', 'center', 'scale', 'knnImpute')) # ignores non-numeric data
		columns_not_in_preprocessed_data <- colnames(classification_train)[!(colnames(classification_train) %in% colnames(pre_processed_numeric_data$data))] # figure out which columns we need to add back in (i.e. all (non-numeric) that are in classification_train but are NOT in pre_processed_numeric_data
		pre_processed_classification_train <- cbind(classification_train[, columns_not_in_preprocessed_data], pre_processed_numeric_data$data)
	
	} else {

		pre_processed_classification_train <- classification_train
	}

	set.seed(custom_seed)
	pre_processed_numeric_data <- preProcess(classification_train, method = c('nzv', 'center', 'scale', 'knnImpute')) # ignores non-numeric data
	columns_not_in_preprocessed_data <- colnames(classification_train)[!(colnames(classification_train) %in% colnames(pre_processed_numeric_data$data))] # figure out which columns we need to add back in (i.e. all (non-numeric) that are in classification_train but are NOT in pre_processed_numeric_data
	pre_processed_classification_train <- cbind(classification_train[, columns_not_in_preprocessed_data], pre_processed_numeric_data$data)

	set.seed(custom_seed)
	stepwise_glm_model <- step(glm(family = binomial, formula = target ~ ., data = pre_processed_classification_train), direction="backward", trace=0) # do stepwise regression (glm doesn't like factor target variables), then use the formula in train in order to take advantage of k-fold Cross Validation
	# stepwise_glm_model$formula gives the `optimal` formula (need to do this because coefficients will have factor variable names, not original columns. The formula will exclude columns not statistically significant)
	# now feed this back into training to do cross validation
	stepwise_glm_model$formula	
	stepwise_logistic_columns <- str_split(stepwise_glm_model$formula, pattern = ' ')[[3]]
	stepwise_logistic_columns <- c('target', stepwise_logistic_columns[stepwise_logistic_columns != '+'])
}

model_pre_processing <- c('nzv', 'center', 'scale')
logistic_regression_stepwise_backward <- train_classification(
	refresh_models = refresh_models,
	training_metric = metric,
	custom_seed = custom_seed,
	model_name = 'logistic_regression_stepwise_backward',
	dataset = classification_train[, stepwise_logistic_columns],
	training_method = 'glm',
	training_preProcess = model_pre_processing,
	training_control = train_control,
	training_tune_grid = NULL,
	training_tune_length = NULL,
	sample_to_predictor_ratio_threshold = 5,
	class_to_predictor_ratio_threshold = NULL,
	class_names = NULL)

model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'logistic_regression_stepwise_backward', training_preProcess = I(list(model_pre_processing))))
classification_models[['logistic_regression_stepwise_backward']] <- logistic_regression_stepwise_backward
```

### linear_discriminant_analsysis

```{r linear_discriminant_analsysis, echo = FALSE, comment = '', results = 'asis'}
model_pre_processing <- c('nzv', 'center', 'scale')
linear_discriminant_analsysis <- train_classification(
	refresh_models = refresh_models,
	training_metric = metric,
	custom_seed = custom_seed,
	model_name = 'linear_discriminant_analsysis',
	dataset = classification_train,
	training_method = 'lda',
	training_preProcess = model_pre_processing,
	training_control = train_control,
	training_tune_grid = NULL,
	training_tune_length = NULL,
	sample_to_predictor_ratio_threshold = 5,
	class_to_predictor_ratio_threshold = NULL,
	class_names = NULL)

model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'linear_discriminant_analsysis', training_preProcess = I(list(model_pre_processing))))
classification_models[['linear_discriminant_analsysis']] <- linear_discriminant_analsysis
```

### linear_discriminant_analsysis_remove_collinear

```{r linear_discriminant_analsysis_remove_collinear, echo = FALSE, comment = '', results = 'asis'}
if(all(classification_column_names %in% recommended_columns_caret)) { # if all the original column names exist in the recommended column names (i.e. columns are the same;  there are no collinear columns) then these results won't differ, we can skip.

	cat('\n\n> No collinear columns removed... skipping.\n\n')
} else {

	model_pre_processing <- c('nzv', 'center', 'scale')
	linear_discriminant_analsysis_remove_collinear <- train_classification(
		refresh_models = refresh_models,
		training_metric = metric,
		custom_seed = custom_seed,
		model_name = 'linear_discriminant_analsysis_remove_collinear',
		dataset = classification_train[, recommended_columns_caret],
		training_method = 'lda',
		training_preProcess = model_pre_processing,
		training_control = train_control,
		training_tune_grid = NULL,
		training_tune_length = NULL,
		sample_to_predictor_ratio_threshold = 5,
		class_to_predictor_ratio_threshold = NULL,
		class_names = NULL)

	model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'linear_discriminant_analsysis_remove_collinear', training_preProcess = I(list(model_pre_processing))))
	classification_models[['linear_discriminant_analsysis_remove_collinear']] <- linear_discriminant_analsysis_remove_collinear
}
```

### linear_discriminant_analsysis_remove_collinear_skew

```{r linear_discriminant_analsysis_remove_collinear_skew, echo = FALSE, comment = '', results = 'asis'}
# need to run this even if no collinear columns are removed since we are also removing any skew, which won't be previously captured
model_pre_processing <- c('nzv', 'YeoJohnson', 'center', 'scale')
linear_discriminant_analsysis_remove_collinear_skew <- train_classification(
	refresh_models = refresh_models,
	training_metric = metric,
	custom_seed = custom_seed,
	model_name = 'linear_discriminant_analsysis_remove_collinear_skew',
	dataset = classification_train[, recommended_columns_caret],
	training_method = 'lda',
	training_preProcess = model_pre_processing,
	training_control = train_control,
	training_tune_grid = NULL,
	training_tune_length = NULL,
	sample_to_predictor_ratio_threshold = 5,
	class_to_predictor_ratio_threshold = NULL,
	class_names = NULL)

model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'linear_discriminant_analsysis_remove_collinear_skew', training_preProcess = I(list(model_pre_processing))))
classification_models[['linear_discriminant_analsysis_remove_collinear_skew']] <- linear_discriminant_analsysis_remove_collinear_skew
```

### partial_least_squares_discriminant_analysis

```{r partial_least_squares_discriminant_analysis, echo = FALSE, comment = '', results = 'asis'}
model_pre_processing <- c('nzv', 'center', 'scale')
partial_least_squares_discriminant_analysis <- train_classification(
	refresh_models = refresh_models,
	training_metric = metric,
	custom_seed = custom_seed,
	model_name = 'partial_least_squares_discriminant_analysis',
	dataset = classification_train,
	training_method = 'pls',
	training_preProcess = model_pre_processing,
	training_control = train_control,
	training_tune_grid = expand.grid(ncomp = tuning_number_of_latent_variables_to_retain),
	training_tune_length = NULL,
	sample_to_predictor_ratio_threshold = 5,
	class_to_predictor_ratio_threshold = NULL,
	class_names = NULL)

model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'partial_least_squares_discriminant_analysis', training_preProcess = I(list(model_pre_processing))))
classification_models[['partial_least_squares_discriminant_analysis']] <- partial_least_squares_discriminant_analysis
```

### partial_least_squares_discriminant_analysis_skew

```{r partial_least_squares_discriminant_analysis_skew, echo = FALSE, comment = '', results = 'asis'}
model_pre_processing <- c('nzv', 'YeoJohnson', 'center', 'scale')
partial_least_squares_discriminant_analysis_skew <- train_classification(
	refresh_models = refresh_models,
	training_metric = metric,
	custom_seed = custom_seed,
	model_name = 'partial_least_squares_discriminant_analysis_skew',
	dataset = classification_train,
	training_method = 'pls',
	training_preProcess = model_pre_processing,
	training_control = train_control,
	training_tune_grid = expand.grid(ncomp = tuning_number_of_latent_variables_to_retain),
	training_tune_length = NULL,
	sample_to_predictor_ratio_threshold = 5,
	class_to_predictor_ratio_threshold = NULL,
	class_names = NULL)

model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'partial_least_squares_discriminant_analysis_skew', training_preProcess = I(list(model_pre_processing))))
classification_models[['partial_least_squares_discriminant_analysis_skew']] <- partial_least_squares_discriminant_analysis_skew
```

### glmnet_lasso_ridge

```{r glmnet_lasso_ridge, echo = FALSE, comment = '', results = 'asis'}
model_pre_processing <- c('nzv', 'center', 'scale')
glmnet_lasso_ridge <- train_classification(
	refresh_models = refresh_models,
	training_metric = metric,
	custom_seed = custom_seed,
	model_name = 'glmnet_lasso_ridge',
	dataset = classification_train,
	training_method = 'glmnet',
	training_preProcess = model_pre_processing,
	training_control = train_control,
	training_tune_grid = expand.grid(alpha = tuning_glmnet_alpha, lambda = tuning_glmnet_lambda),
	training_tune_length = NULL,
	sample_to_predictor_ratio_threshold = 5,
	class_to_predictor_ratio_threshold = NULL,
	class_names = NULL)

model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'glmnet_lasso_ridge', training_preProcess = I(list(model_pre_processing))))
classification_models[['glmnet_lasso_ridge']] <- glmnet_lasso_ridge
```

### sparse_lda

```{r sparse_lda, echo = FALSE, comment = '', results = 'asis'}
model_pre_processing <- c('nzv', 'center', 'scale')
sparse_lda <- train_classification(
	refresh_models = refresh_models,
	training_metric = metric,
	custom_seed = custom_seed,
	model_name = 'sparse_lda',
	dataset = classification_train,
	training_method = 'sparseLDA',
	training_preProcess = model_pre_processing,
	training_control = train_control,
	training_tune_grid = NULL,
	training_tune_length = 5,
	sample_to_predictor_ratio_threshold = 5,
	class_to_predictor_ratio_threshold = NULL,
	class_names = NULL)

model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'sparse_lda', training_preProcess = I(list(model_pre_processing))))
classification_models[['sparse_lda']] <- sparse_lda
```

### nearest_shrunken_centroids

> was causing an error, turned off

```{r nearest_shrunken_centroids, eval=FALSE, include=FALSE, comment = '', results = 'asis'}
model_pre_processing <- c('nzv', 'center', 'scale')
nearest_shrunken_centroids <- train_classification(
	refresh_models = refresh_models,
	training_metric = metric,
	custom_seed = custom_seed,
	model_name = 'nearest_shrunken_centroids',
	dataset = classification_train,
	training_method = 'pam',
	training_preProcess = model_pre_processing,
	training_control = train_control,
	training_tune_grid = tuning_nearest_shrunken_centroids_shrinkage_threshold,
	training_tune_length = NULL,
	sample_to_predictor_ratio_threshold = 5,
	class_to_predictor_ratio_threshold = NULL,
	class_names = NULL)

model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'nearest_shrunken_centroids', training_preProcess = I(list(model_pre_processing))))
classification_models[['nearest_shrunken_centroids']] <- nearest_shrunken_centroids
```

### regularized_discriminant_analysis

```{r regularized_discriminant_analysis, echo = FALSE, comment = '', results = 'asis'}
model_pre_processing <- c('nzv', 'center', 'scale')
regularized_discriminant_analysis <- train_classification(
	refresh_models = refresh_models,
	training_metric = metric,
	custom_seed = custom_seed,
	model_name = 'regularized_discriminant_analysis',
	dataset = classification_train,
	training_method = 'rda',
	training_preProcess = model_pre_processing,
	training_control = train_control,
	training_tune_grid = expand.grid(lambda = tuning_rda_lambda, gamma = tuning_rda_gamma),
	training_tune_length = NULL,
	sample_to_predictor_ratio_threshold = NULL,
	class_to_predictor_ratio_threshold = 5,
	class_names = c(target_positive_class, target_negative_class))

model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'regularized_discriminant_analysis', training_preProcess = I(list(model_pre_processing))))
classification_models[['regularized_discriminant_analysis']] <- regularized_discriminant_analysis
```

### regularized_discriminant_analysis_rc

```{r regularized_discriminant_analysis_rc, echo = FALSE, comment = '', results = 'asis'}
if(all(classification_column_names %in% recommended_columns_caret)) { # if all the original column names exist in the recommended column names (i.e. columns are the same;  there are no collinear columns) then these results won't differ, we can skip.

	cat('\n\n> No collinear columns removed... skipping.\n\n')
} else {

	model_pre_processing <- c('nzv', 'center', 'scale')
	regularized_discriminant_analysis_rc <- train_classification(
		refresh_models = refresh_models,
		training_metric = metric,
		custom_seed = custom_seed,
		model_name = 'regularized_discriminant_analysis_rc',
		dataset = classification_train[, recommended_columns_caret],
		training_method = 'rda',
		training_preProcess = model_pre_processing,
		training_control = train_control,
		training_tune_grid = expand.grid(lambda = tuning_rda_lambda, gamma = tuning_rda_gamma),
		training_tune_length = NULL,
		sample_to_predictor_ratio_threshold = NULL,
		class_to_predictor_ratio_threshold = 5,
		class_names = c(target_positive_class, target_negative_class))

	model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'regularized_discriminant_analysis_rc', training_preProcess = I(list(model_pre_processing))))
	classification_models[['regularized_discriminant_analysis_rc']] <- regularized_discriminant_analysis_rc
}
```

### mixture_discriminant_analysis

```{r mixture_discriminant_analysis, echo = FALSE, comment = '', results = 'asis'}
model_pre_processing <- c('nzv', 'center', 'scale')
mixture_discriminant_analysis <- train_classification(
	refresh_models = refresh_models,
	training_metric = metric,
	custom_seed = custom_seed,
	model_name = 'mixture_discriminant_analysis',
	dataset = classification_train,
	training_method = 'mda',
	training_preProcess = model_pre_processing,
	training_control = train_control,
	training_tune_grid = expand.grid(subclasses = tuning_mda_subclasses),
	training_tune_length = NULL,
	sample_to_predictor_ratio_threshold = NULL,
	class_to_predictor_ratio_threshold = 5,
	class_names = c(target_positive_class, target_negative_class))

model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'mixture_discriminant_analysis', training_preProcess = I(list(model_pre_processing))))
classification_models[['mixture_discriminant_analysis']] <- mixture_discriminant_analysis
```

### mixture_discriminant_analysis_rc

```{r mixture_discriminant_analysis_rc, echo = FALSE, comment = '', results = 'asis'}
if(all(classification_column_names %in% recommended_columns_caret)) { # if all the original column names exist in the recommended column names (i.e. columns are the same;  there are no collinear columns) then these results won't differ, we can skip.

	cat('\n\n> No collinear columns removed... skipping.\n\n')
} else {

	model_pre_processing <- c('nzv', 'center', 'scale')
	mixture_discriminant_analysis_rc <- train_classification(
		refresh_models = refresh_models,
		training_metric = metric,
		custom_seed = custom_seed,
		model_name = 'mixture_discriminant_analysis_rc',
		dataset = classification_train[, recommended_columns_caret],
		training_method = 'mda',
		training_preProcess = model_pre_processing,
		training_control = train_control,
		training_tune_grid = expand.grid(subclasses = tuning_mda_subclasses),
		training_tune_length = NULL,
		sample_to_predictor_ratio_threshold = NULL,
		class_to_predictor_ratio_threshold = 5,
		class_names = c(target_positive_class, target_negative_class))

	model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'mixture_discriminant_analysis_rc', training_preProcess = I(list(model_pre_processing))))
	classification_models[['mixture_discriminant_analysis_rc']] <- mixture_discriminant_analysis_rc
}
```

### neural_network_spatial_rc

```{r neural_network_spatial_rc, echo = FALSE, comment = '', results = 'asis'}
parameter_nnet_training_numWts <- calculate_nnet_MaxNWts(max_nnet_size = max(tuning_nnet_size), number_of_dataset_columns = ncol(classification_train[, recommended_columns_caret]))

model_pre_processing <- c('nzv', 'center', 'scale', 'spatialSign')
neural_network_spatial_rc <- train_classification(
	refresh_models = refresh_models,
	training_metric = metric,
	custom_seed = custom_seed,
	model_name = 'neural_network_spatial_rc',
	dataset = classification_train[, recommended_columns_caret],
	training_method = 'nnet',
	training_preProcess = model_pre_processing,
	training_control = train_control,
	training_tune_grid = expand.grid(size = tuning_nnet_size, decay = tuning_nnet_decay),
	training_tune_length = NULL,
	sample_to_predictor_ratio_threshold = NULL,
	class_to_predictor_ratio_threshold = 5,
	class_names = c(target_positive_class, target_negative_class),
	neural_net_numWts = parameter_nnet_training_numWts,
	neural_net_lineout = parameter_nnet_linout,
	neural_net_max_iterations = parameter_nnet_max_iterations)

model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'neural_network_spatial_rc', training_preProcess = I(list(model_pre_processing))))
classification_models[['neural_network_spatial_rc']] <- neural_network_spatial_rc
```

### neural_network_spatial_rc_skew

```{r neural_network_spatial_rc_skew, echo = FALSE, comment = '', results = 'asis'}
parameter_nnet_training_numWts <- calculate_nnet_MaxNWts(max_nnet_size = max(tuning_nnet_size), number_of_dataset_columns = ncol(classification_train[, recommended_columns_caret]))

model_pre_processing <- c('nzv', 'YeoJohnson', 'center', 'scale', 'spatialSign')
neural_network_spatial_rc_skew <- train_classification(
	refresh_models = refresh_models,
	training_metric = metric,
	custom_seed = custom_seed,
	model_name = 'neural_network_spatial_rc_skew',
	dataset = classification_train[, recommended_columns_caret],
	training_method = 'nnet',
	training_preProcess = model_pre_processing,
	training_control = train_control,
	training_tune_grid = expand.grid(size = tuning_nnet_size, decay = tuning_nnet_decay),
	training_tune_length = NULL,
	sample_to_predictor_ratio_threshold = NULL,
	class_to_predictor_ratio_threshold = 5,
	class_names = c(target_positive_class, target_negative_class),
	neural_net_numWts = parameter_nnet_training_numWts,
	neural_net_lineout = parameter_nnet_linout,
	neural_net_max_iterations = parameter_nnet_max_iterations)

model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'neural_network_spatial_rc_skew', training_preProcess = I(list(model_pre_processing))))
classification_models[['neural_network_spatial_rc_skew']] <- neural_network_spatial_rc_skew
```

### flexible_discriminant_analsysis

```{r flexible_discriminant_analsysis, echo = FALSE, comment = '', results = 'asis'}
model_pre_processing <- c('nzv')
flexible_discriminant_analsysis <- train_classification(
	refresh_models = refresh_models,
	training_metric = metric,
	custom_seed = custom_seed,
	model_name = 'flexible_discriminant_analsysis',
	dataset = classification_train,
	training_method = 'fda',
	training_preProcess = model_pre_processing,
	training_control = train_control,
	training_tune_grid = NULL,
	training_tune_length = 10,
	sample_to_predictor_ratio_threshold = NULL,
	class_to_predictor_ratio_threshold = 5,
	class_names = c(target_positive_class, target_negative_class))

model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'flexible_discriminant_analsysis', training_preProcess = I(list(model_pre_processing))))
classification_models[['flexible_discriminant_analsysis']] <- flexible_discriminant_analsysis
```

### svm_linear

```{r svm_linear, echo = FALSE, comment = '', results = 'asis'}
#modelLookup('svmLinear')
#getModelInfo('svmLinear')

model_pre_processing <- c('center', 'scale')
svm_linear <- train_classification(
	refresh_models = refresh_models,
	training_metric = metric,
	custom_seed = custom_seed,
	model_name = 'svm_linear',
	dataset = classification_train,
	training_method = 'svmLinear2',
	training_preProcess = model_pre_processing,
	training_control = train_control,
	training_tune_grid = NULL,
	training_tune_length = tuning_svm_linear_num_costs,
	sample_to_predictor_ratio_threshold = NULL,
	class_to_predictor_ratio_threshold = 5,
	class_names = c(target_positive_class, target_negative_class))

model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'svm_linear', training_preProcess = I(list(model_pre_processing))))
classification_models[['svm_linear']] <- svm_linear
```

### svm_polynomial

```{r svm_polynomial, echo = FALSE, comment = '', results = 'asis'}
#modelLookup('svmPoly')
#getModelInfo('svmPoly')

model_pre_processing <- c('center', 'scale')
svm_polynomial <- train_classification(
	refresh_models = refresh_models,
	training_metric = metric,
	custom_seed = custom_seed,
	model_name = 'svm_polynomial',
	dataset = classification_train,
	training_method = 'svmPoly',
	training_preProcess = model_pre_processing,
	training_control = train_control,
	training_tune_grid = NULL,
	training_tune_length = tuning_svm_poly_num_costs,
	sample_to_predictor_ratio_threshold = NULL,
	class_to_predictor_ratio_threshold = 5,
	class_names = c(target_positive_class, target_negative_class))

model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'svm_polynomial', training_preProcess = I(list(model_pre_processing))))
classification_models[['svm_polynomial']] <- svm_polynomial
```

### svm_radial

```{r svm_radial, echo = FALSE, comment = '', results = 'asis'}
#modelLookup('svmRadial')
#getModelInfo('svmRadial')

model_pre_processing <- c('center', 'scale')
svm_radial <- train_classification(
	refresh_models = refresh_models,
	training_metric = metric,
	custom_seed = custom_seed,
	model_name = 'svm_radial',
	dataset = classification_train,
	training_method = 'svmRadial',
	training_preProcess = model_pre_processing,
	training_control = train_control,
	training_tune_grid = NULL,
	training_tune_length = tuning_svm_radial_num_costs,
	sample_to_predictor_ratio_threshold = NULL,
	class_to_predictor_ratio_threshold = 5,
	class_names = c(target_positive_class, target_negative_class))

model_preprocessing_values <- rbind(model_preprocessing_values, data.frame(model = 'svm_radial', training_preProcess = I(list(model_pre_processing))))
classification_models[['svm_radial']] <- svm_radial
```

### Random Forest

### Neural Network

### Ada Boost

### All Models on Page 550 that are classification or both regression and classification

### Models used for spot-check.Rmd 


## Resamples & Top Models

## Resamples

```{r resamples_regression, echo=FALSE, fig.height=6, fig.width=9}
resamples <- resamples(classification_models)

# Table comparison
(resamples_summary <- summary(resamples))

resamples_long <- resamples$values %>%
	dplyr::select(-Resample) %>%
	gather(type, value) %>%
	separate(col=type, into=c('model', 'metric'), sep = '~') %>%
	mutate(model = factor(model, levels = rev(resamples$models)), metric = factor(metric, levels = c('ROC', 'Sens', 'Spec')))

resamples_means <- resamples_long %>%
	dplyr::group_by(model, metric) %>%
	dplyr::summarise(average = mean(value)) %>%
	dplyr::ungroup()

max_roc_mean <- max(resamples_means %>% filter(metric == 'ROC') %>% dplyr::select(average))
max_sensitivity_mean <- max(resamples_means %>% filter(metric == 'Sens') %>% dplyr::select(average))
max_specificity_mean <- max(resamples_means %>% filter(metric == 'Spec') %>% dplyr::select(average))

best_values <- data.frame(metric = levels(resamples_long$metric), Z = c(max_roc_mean, max_sensitivity_mean, max_specificity_mean))
ggplot(data = resamples_long, mapping = aes(x = model, y = value)) +
	geom_boxplot() + 
	stat_summary(fun.y=mean, colour="red", geom="point", shape=21, size=3, show.legend = FALSE) + 
	geom_hline(data = best_values, aes(yintercept = Z), color='red') +
	facet_grid(. ~ metric, scales = "free", space = "free_y") +
	coord_flip()

resamples$values %>% dplyr::select(contains('~ROC')) %>%
	gather(model, ROC) %>%
	mutate(model = factor(model)) %>%
ggplot() + 
	geom_density(aes(ROC, color = model)) + 
    geom_jitter(aes(ROC, 0, color = model), alpha = 0.5, height = 0.02) 

# densityplot(resamples, metric = 'ROC')
densityplot(resamples, metric = 'Sens')
densityplot(resamples, metric = 'Spec')
```

## Train Top Models on Entire Training Dataset & Predict on Test Set

> after using cross-validation to tune, we will take the highest ranked models, retrain the models (with the final tuning parameters) on the entire training set, and predict using the test set.

```{r model_definitions, echo=FALSE}
model_plots <- function(model, testing_set, prediction_threshold = 0.5){

	predictions_probabilities_raw <- predict(model, testing_set %>% dplyr::select(-target), type = 'prob')
	actual_observations <- testing_set$target
	predicted_probabilities_negative <- predictions_probabilities_raw[, target_negative_class]
	predicted_probabilities_positive <- predictions_probabilities_raw[, target_positive_class]

	names(predicted_probabilities_negative) <- NULL
	names(predicted_probabilities_positive) <- NULL

	df_results <- data.frame(	actual_observations = actual_observations,
						 		predicted_probabilities = predicted_probabilities_positive,
						 		predicted_probabilities_negative = predicted_probabilities_negative)
	df_results$predicted_class <- predict(model, testing_set %>% dplyr::select(-target))
	df_results$label <- ifelse(actual_observations == target_negative_class,
									paste('True Outcome:', target_negative_class), paste('True Outcome:', target_positive_class))

	# prop.table(table(classification_test$target))[target_positive_class]
	# ggplot(df_results, aes(predicted_probabilities)) + geom_histogram(binwidth = 0.1)

	### Plot the probability of negative/positive
	print(histogram(~predicted_probabilities_negative | label, data = df_results, layout = c(2, 1), nint = 20, xlab = paste0('Probability of `', target_negative_class,'`'), type = 'count'))
	print(histogram(~predicted_probabilities | label, data = df_results, layout = c(2, 1), nint = 20, xlab = paste0('Probability of `', target_positive_class,'`'), type = 'count'))

	### Create the confusion matrix from the test set.
	confusion_matrix <- table(actual = actual_observations, predictions = df_results$predicted_class)
	confusion_ls <- confusion_list(true_pos = confusion_matrix[target_positive_class, target_positive_class],
									true_neg = confusion_matrix[target_negative_class, target_negative_class],
									false_pos = confusion_matrix[target_negative_class, target_positive_class],
									false_neg = confusion_matrix[target_positive_class, target_negative_class])
	observation_rates <- confusion_matrix / sum(confusion_matrix)

	average_profit <- 5
	average_cost <- 2
	gain_true_positive <- average_profit - average_cost # positive is gain
	gain_true_negative <- 0
	gain_false_positive <- -average_cost # negative is cost
	gain_false_negative <- -(average_profit - average_cost) # more than likely, opportunity cost, but still should be counted (opportunity cost of not making the profit, but had we labed as a positive, we would have also incurred the average_cost (e.g. of mailing the letter, etc.))
	gain_cost_matrix <- matrix(c(gain_true_negative, gain_false_positive, gain_false_negative, gain_true_positive), nrow = 2, byrow = TRUE, dimnames = list(c('Actual Negative', 'Actual Positive'), c('Predicted Negative', 'Predicted Positive')))

	print(expected_value_chart(predicted_probabilities_positive = predicted_probabilities_positive, actual_outcomes = actual_observations == target_positive_class, gain_cost_matrix = gain_cost_matrix))

	print(visualize_quality_of_model(confusion_ls))
	conf_matrix <- confusionMatrix(data = df_results$predicted_class, reference = df_results$actual_observations)
	cat('\n\n')
	cat(codebc(conf_matrix, postfix = '\n\n'))

	### ROC curves:
	roc_pred <- prediction(predictions = predicted_probabilities_positive, labels = actual_observations)
	roc_perf <- performance(roc_pred, measure = 'sens', x.measure = 'fpr')
	print(plot(roc_perf, colorize = TRUE, print.cutoffs.at = seq(0,1,0.05), text.adj = c(-0.2, 1.7)))

	# calculate AUC
	perf_auc <- performance(roc_pred, measure = 'auc')
	model_auc <- unlist(perf_auc@y.values)

	# Gain/Lift charts
	creditLift <- lift(actual_observations ~ predicted_probabilities_positive, data = df_results)
	print(xyplot(creditLift))

	gl_table <- gain_lift_table(actual_observations = actual_observations, predicted_probabilities = predicted_probabilities_positive, target_positive_class = target_positive_class)
	gl_charts <- gain_lift_charts(gl_table = gl_table)

	print(gl_charts[[1]])
	print(gl_charts[[2]])

	# Calibration Chart
	calibration_data <- calibration(actual_observations ~ predicted_probabilities, data = df_results, cuts = 10)
	print(xyplot(calibration_data, auto.key = list(columns = 2)))

	cal_table <- calibration_table(actual_observations = actual_observations, predicted_probabilities = predicted_probabilities_positive, target_positive_class = target_positive_class)
	cal_chart <- calibration_chart(cal_table = cal_table)
	print(cal_chart)

	return (c(model_auc = model_auc, model_sensitivity = as.numeric(conf_matrix$byClass['Sensitivity']), model_specificity = as.numeric(conf_matrix$byClass['Specificity'])))
}
```

```{r top_models, echo=FALSE, fig.height=5, fig.width=8, comment = '', results = 'asis'}
stopifnot(length(classification_models) == nrow(model_preprocessing_values))

top_models <- resamples_means %>% 
	filter(metric == 'ROC') %>% 
	mutate(rank = dense_rank(desc(average)),
		   model = as.character(model)) %>% 
	filter(rank <= 5) %>% # HIGHEST AUC/ROC
	arrange(rank) %>% 
	dplyr::select(model, average) %>%
	dplyr::rename(cross_validation_auc = average) %>%
	mutate(model_object = map(model, ~ { classification_models[names(classification_models) == .][[1]] })) # add models to data.frame so when we choose the top models later we will 

expected_rows <- nrow(top_models)
top_models <- inner_join(top_models, model_preprocessing_values %>% mutate(model = as.character(model)), by = 'model')

stopifnot(nrow(top_models) == expected_rows)
stopifnot(!is.na(top_models$cross_validation_auc))
stopifnot(!is.na(top_models$model_object))

final_train_control <- trainControl(method = "none", summaryFunction = twoClassSummary, classProbs = TRUE, savePredictions = 'all') # In cases where the model tuning values are known, train can be used to fit the model to the entire training set without any resampling or parameter tuning. Using the method = "none" option in trainControl can be used https://topepo.github.io/caret/model-training-and-tuning.html#fitting-models-without-parameter-tuning

model_stats <- pmap(list(as.character(top_models$model), top_models$model_object, top_models$training_preProcess), function(.x, .y, .z){
	# model_name <- as.character(top_models$model[1])
	# model_object <- top_models$model_object[[1]]
	# pre_process <- top_models$training_preProcess[[1]]
	model_name <- .x
	model_object <- .y
	pre_process <- .z
	
	cat(paste0('\n\n### ', model_object$modelInfo$label, ' (', model_name,')\n\n'))

	cat(paste0('\n\n> Model Processing: ', paste0(pre_process, collapse = '; ')), '\n\n')
	if(any(is.na(pre_process))) { # when we set the values (i.e. rbind to dataframe, we have to use NA, but when we pass it in, it should be NULL)

		pre_process = NULL
	}

	# get method, best tuning grid/parameters, pre-processing steps.
	model_method <- model_object$method
	tune_grid <- model_object$bestTune

	# some models only use a subset of the columns; get the columns used and transform into a formula
	training_column_names <- colnames(model_object$trainingData)
	model_formula_string <- paste('target ~', paste0(training_column_names[-which(training_column_names == '.outcome')], collapse = ' + '))
	model_formula <- as.formula(model_formula_string)
	cat(paste0('\n\n> Model Formula: `', model_formula_string, '`\n\n'))

	if(refresh_models)
	{
		# some models have different parameters for `train`, which we need to use when training the final model
		if(model_method == 'nnet')
		{

			parameter_nnet_training_numWts <- calculate_nnet_MaxNWts(max_nnet_size = max(tune_grid$size), number_of_dataset_columns = length(training_column_names))
			cat(paste0('\n\n> Setting `nnet` specific parameters: `MaxNWts` - `', parameter_nnet_training_numWts,'`; `maxit` - `', parameter_nnet_max_iterations,'`\n\n'))
			
			# seems to fail if the number of calculated weights exceeds the MaxWts parameter, so we'll set it to a default minimum
			parameter_nnet_training_numWts <- max(parameter_nnet_training_numWts, 50)
			set.seed(custom_seed)
			final_model <- train(	model_formula,
									data = classification_train,
									method = model_method,
									metric = metric,
									preProc = pre_process,
									trControl = final_train_control,
									tuneGrid = tune_grid,
									trace = FALSE,
									maxit = parameter_nnet_max_iterations,
									MaxNWts = parameter_nnet_training_numWts,
									linout = parameter_nnet_linout)
			
  			#final_model <- randomForest(model_formula, data = classification_train, ntree = 2000)

		}else{
			set.seed(custom_seed)
			final_model <- train(	model_formula,
									data = classification_train,
									method = model_method,
									metric = metric,
									preProc = pre_process,
									trControl = final_train_control,
									tuneGrid = tune_grid)
		}

		saveRDS(final_model, file = paste0('./classification_data/top_model_', model_name, '.RDS'))

	}else{

		final_model <- readRDS(file = paste0('./classification_data/top_model_', model_name, '.RDS'))
	}

	expected_error = tryCatch(
		cat(codebc(summary(final_model$finalModel), postfix='\n\n')),# this causes an error with some models
		error=function(e) e)
	
	model_stats <- model_plots(model = final_model, testing_set = classification_test)
	return (model_stats)
})

final_models_metrics <- data.frame(model = factor(as.character(top_models$model), levels = as.character(top_models$model)),
								   test_set_auc = map_dbl(model_stats, ~ .['model_auc']),
								   cross_validation_auc = top_models$cross_validation_auc,
								   sensitivity = map_dbl(model_stats, ~ .['model_sensitivity']),
								   specificity = map_dbl(model_stats, ~ .['model_specificity']))

max_test_auc <- max(final_models_metrics$test_set_auc)
max_sensitivity <- max(final_models_metrics$sensitivity)
max_specificity <- max(final_models_metrics$specificity)

#prepare/format data for ggplot
final_models_metrics_long <- gather(final_models_metrics, metric, value, -model) %>%
	mutate(model = factor(model, levels = rev(unique(model))),
		type = ifelse(str_detect(metric, 'auc'), str_split(metric, '_'), '')) %>%
	mutate(type = map_chr(type, ~ {
			if(length(.) > 1)
			{
				return(paste0(.[1:2], collapse = ' '))
			}else{
				return ('test set')
			}
		}),
	metric = ifelse(str_detect(metric, 'auc'), 'auc', as.character(metric))) %>%
	mutate(metric = factor(metric, levels = unique(metric)))

best_values <- data.frame(metric = unique(final_models_metrics_long$metric), Z = c(max_test_auc, max_sensitivity, max_specificity))
ggplot(data = final_models_metrics_long, mapping = aes(x = model, y = value, col = type)) +
	geom_point(size = 3, alpha = 0.7) +
	geom_hline(data = best_values, aes(yintercept = Z), color='red') +
	facet_grid(. ~ metric, scales = "free", space = "free_y") +
	coord_flip()
```
