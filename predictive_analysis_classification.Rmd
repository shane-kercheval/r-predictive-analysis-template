---
title: "Predictive Analysis"
author: "Shane Kercheval"
output:
  md_document:
    variant: markdown_github
    toc: true
    toc_depth: 3
---

```{r setup, echo = FALSE, results='hide', message=FALSE, warning=FALSE}
options(scipen=999) # non-scientific notation
options(width=180)
source('~/r-tools/output/plots.R', chdir=TRUE)
source('~/r-tools/output/markdown.R', chdir=TRUE)
source('~/r-tools/general/model_measurements.R', chdir=TRUE)
library('data.table')
library(partykit) # Use the partykit package to make some nice plots. First, convert the rpart objects to party objects.
library(psych)
library(knitr)
library(scales)
library(DMwR)
library(e1071)
library(Hmisc)
library(corrplot)
library(stringr)
library(tidyverse)
library(reshape2)
library(caret)
library(gmodels)
library(randomForest)
library(ROCR)

opts_chunk$set(out.width='750px', dpi=200)

require(doMC)
registerDoMC(cores = round(detectCores()/2) + 1) # All subsequent models are then run in parallel - https://topepo.github.io/caret/parallel-processing.html

custom_seed <- 123
refresh_models <- TRUE
```

```{r function_definition, echo = FALSE}
check_data <- function(dataset, validate_n_p = TRUE)
{
	if(validate_n_p) {
		number_of_samples <- nrow(dataset)
		number_of_dimensions <- ncol(dataset)
		sample_to_dimension_ratio <- number_of_samples / number_of_dimensions

		stopifnot(sample_to_dimension_ratio >= 5)
	}
}
```

```{r dataset_definition, echo = FALSE}
df_credit <- fread('./classification_data/credit.csv')
classification_dataset <- df_credit %>%
	dplyr::rename(target = default) %>%
	dplyr::mutate(phone = ifelse(phone == 'yes', TRUE, FALSE)) %>% # technically don't need to do this but there seems to be an error in dplyr.mutate_if https://github.com/tidyverse/dplyr/pull/2011 which says it is resolved, but still seems to be a problem, dispite having latest version
	dplyr::select(target, everything())
classification_dataset <- as.data.frame(classification_dataset)

target_positive_class <- 'yes'
target_negative_class <- 'no'
```

```{r dataset_prepare, echo = FALSE}
classification_dataset$target <- factor(classification_dataset$target, levels = c(target_positive_class, target_negative_class))

classification_dataset <- classification_dataset %>%
	dplyr::mutate_if(is.character, as.factor) %>%
	dplyr::mutate_if(is.logical, as.factor)

classification_column_names <- colnames(classification_dataset)
numeric_columns <- map_lgl(classification_column_names, ~ {
	return (is.numeric(classification_dataset[, .]))
})
numeric_column_names <- classification_column_names[numeric_columns]

num_sample_size <- nrow(classification_dataset)
num_predictors <- length(classification_dataset) - 1
```

# Tuning Parameters

```{r tuning_parameters}
# train/test set
training_percentage <- 0.90

# cross validation
cross_validation_num_folds <- 10
cross_validation_num_repeats <- 3

# tuning parameters
tuning_number_of_latent_variables_to_retain <- 1:10

tuning_glmnet_alpha <- seq(from = 0, to = 1, length = 5) # alpha = 0 is pure ridge regression, and alpha = 1 is pure lasso regression.
tuning_glmnet_lambda <- seq(from = 0.0001, to = 1, length = 50) # lambda values control the amount of penalization in the model.

tuning_nearest_shrunken_centroids_shrinkage_threshold <- data.frame(.threshold = 0:25)
```

# Dataset

> Assumes the dataset has factors for strings; logical for TRUE/FALSE; `target` for outcome variable

## Summary

> Total predictors: ``r num_predictors``

> Total data-points/rows: ``r num_sample_size``

> Number of training data-points: ``r round(num_sample_size * training_percentage)``

Rule of thumbs for dimensions (Probabilistic and Statistical Modeling in Computer Science; pg 430):

> r < sqrt(n); where r is the number of predictors and sqrt(n) is the square root of the sample size (``r round(sqrt(num_sample_size))``): ``r num_predictors < round(sqrt(num_sample_size))``

> r < sqrt(n_t); where r is the number of predictors and sqrt(n_t) is the square root of the training set size (``r round(sqrt(round(num_sample_size * training_percentage)))``): ``r num_predictors < round(sqrt(round(num_sample_size * training_percentage)))``

```{r summary, echo = FALSE}
summary(classification_dataset)
```

## Skewness

Note: `Box-Cox` can only be applied to sets (i.e. predictors) where all values are `> 0`. So some/most/all? `NA`s will be from that limiation.

```{r skewness, echo = FALSE}
skewnewss_statistics <- map_dbl(classification_column_names, ~ {
	column_name <- .

	dataset_column <- classification_dataset[, column_name]
	if(is.numeric(dataset_column) && min(dataset_column > 0))
	{
		return (BoxCoxTrans(dataset_column)$skewness)
	}else{
		return (NA)
	}
})

kable(data.frame(column = classification_column_names, boxcox_skewness = skewnewss_statistics))
```

## Outliers

```{r outliers, echo = FALSE}
df_outliers <- map(numeric_column_names, ~ {
	numeric_data <- classification_dataset[, .]

	outlier_range <- 2 # this multipied by IQR (traditionally this number is 1.5)

	lower_quantile <- quantile(numeric_data)[2]
	upper_quantile <- quantile(numeric_data)[4]
	iqr <- upper_quantile - lower_quantile

	threshold_upper <- (iqr * outlier_range) + upper_quantile
	threshold_lower <- lower_quantile - (iqr * 1.5) # Any numeric_data point outside (> threshold_upper or < threshold_lower) these values is a mild outlier

	upper_outlier_count <- sum(numeric_data > threshold_upper)
	lower_outlier_count <- sum(numeric_data < threshold_lower)

	return (list(upper_outlier_count, lower_outlier_count))
})

df_outliers <- data.frame(columns = numeric_column_names, lower_outlier_count = map_chr(df_outliers, ~ {.[[2]]}), upper_outlier_count = map_chr(df_outliers, ~ {.[[1]]}))
kable(df_outliers)
```

## Correlation & Collinearity

### Correlation

```{r correlation, echo = FALSE, fig.height=5, fig.width=5}
#pairs.panels(classification_dataset[, numeric_columns])
matrix_correlations <- cor(classification_dataset[, numeric_columns])
corrplot::corrplot(matrix_correlations, order = 'hclust', tl.cex = .35, col = colorRampPalette(rev(c('#67001F', '#B2182B', '#D6604D', '#F4A582', '#FDDBC7', '#FFFFFF', '#D1E5F0', '#92C5DE', '#4393C3', '#2166AC', '#053061')))(200))
```

### Collinearity Removal

```{r removal_setup, echo = FALSE}
correlation_threshold <- 0.90
```

#### Caret's `findCorrelation`

Shows caret's recommendation of removing collinear columns based on correlation threshold of ``r correlation_threshold``

```{r caret_collinearity, echo = FALSE}
numeric_predictor_data <- classification_dataset[, numeric_columns]
collinear_indexes <- findCorrelation(cor(numeric_predictor_data), correlation_threshold)
if(length(collinear_indexes) == 0)
{
	recommended_columns_caret <- colnames(classification_dataset)
}else{
	recommended_columns_caret <- c('target', colnames(numeric_predictor_data)[-collinear_indexes])	
}
```

> columns recommended for removal: ``r paste(colnames(numeric_predictor_data)[collinear_indexes], collapse = ', ')``

> final columns recommended: ``r paste(recommended_columns_caret, collapse = ', ')``

#### Heuristic

This method is described in APM pg 47 as the following steps

- calculate the correlation matrix of predictors
- determine the two predictors associated with the largest absolute pairwise correlation (call them predictors `A` and `B`)
- Determine the average correlation between `A` and the other variables.
	- Do the same for `B`
- If `A` has a larger average correlation, remove it; otherwise, remove predcitor `B`
- Repeat until no absolute correlations are above the threshold (``r correlation_threshold``)

```{r heuristic_collinearity, echo = FALSE}
predictor_correlations <- cor(numeric_predictor_data)

walk(1:nrow(predictor_correlations), ~ {
	predictor_correlations[.,.] <<- NA	
})

while(max(abs(predictor_correlations), na.rm = TRUE) > correlation_threshold) {
	## caret's findCorrelation function is used to identify columns to remove.
	num_cols_rows <- nrow(predictor_correlations)
	#highCorr <- findCorrelation(predictor_correlations, .75)
	ab_index <- which(abs(predictor_correlations) == max(abs(predictor_correlations), na.rm = TRUE) & abs(predictor_correlations) > 0.75)[1] # get the first index of the highest correlation
	a_index <- ceil(ab_index / num_cols_rows) # get the row number 
	b_index <- ab_index - (num_cols_rows * (a_index-1))

	ave_a_predictor_correlations <- mean(abs(predictor_correlations[a_index, ]), na.rm = TRUE)
	ave_b_predictor_correlations <- mean(abs(predictor_correlations[b_index, ]), na.rm = TRUE)

	if(ave_a_predictor_correlations > ave_b_predictor_correlations)
	{
		predictor_correlations <<- predictor_correlations[-a_index, -a_index]
				
	}else
	{
		predictor_correlations <<- predictor_correlations[-b_index, -b_index]
	}	
	
}
recommended_numeric_columns <- colnames(predictor_correlations)
non_numeric_columns <- classification_column_names[!(classification_column_names %in% numeric_column_names)] # includes target
recommended_columns_custom <- c(recommended_numeric_columns, non_numeric_columns)
recommended_removal <- classification_column_names[!(classification_column_names %in% recommended_columns_custom)]
```

> columns recommended for removal: ``r paste(recommended_removal, collapse = ', ')``

> final columns recommended: ``r paste(recommended_columns_custom, collapse = ', ')``

## Graphs

```{r graphs, echo = FALSE, fig.height=4, fig.width=6, comment='', results='asis'}
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

colors_two = rev(gg_color_hue(2))

walk(classification_column_names, ~ {
	column_name <- .
	
	if(column_name == 'target')
	{
		return ()
	}

	cat(paste('\n\n###', column_name, '\n\n'))
	dataset_column <- classification_dataset[, column_name]
	if(is.numeric(dataset_column))
	{
		df_data <- data.frame(target=dataset_column)
		fun_args <- list(mean = mean(df_data$target), sd = sd(df_data$target))
		quantiles <- quantile(df_data$target, c(0.02, 0.98)) # cut off any 'outliers' (crude) so it doesn't skew graph
		
		result = tryCatch({
			print(ggplot(data = df_data, mapping = aes(x = target)) +
				geom_histogram(mapping=aes(y = ..density..), bins = 30) + # histogram of data
				geom_density(col = 'red') + # emperical PDF
				stat_function(fun = dnorm, args = fun_args, col = 'blue') + # theoretical normal PDF, based on data's mean & standard deviation
				coord_cartesian(xlim = c(quantiles[1], quantiles[2])) + labs(x = column_name))

		}, error = function(e) {
			cat('\n\nError with histogram.\n\n')
		})

		print(ggplot(data = df_data, mapping = aes(x = column_name,  y = target, group = 1)) +
				geom_boxplot() +
				xlab(column_name) + ylab(column_name))

		print(ggplot(data = classification_dataset, mapping = aes(x = target, y = dataset_column)) +
				geom_boxplot() + 
				xlab('target') + ylab(column_name))
		
		if(length(levels(classification_dataset$target)) == 2){
			levels <- levels(classification_dataset$target)
			level_1 <- levels[1]
			level_2 <- levels[2]

			level_1_values <- (classification_dataset %>% filter(target == level_1))[, column_name]
			level_2_values <- (classification_dataset %>% filter(target == level_2))[, column_name]

			#https://www.r-bloggers.com/two-sample-students-t-test-1/
			# Before proceeding with the t-test, it is necessary to evaluate the sample variances of the two groups, using a Fisher’s F-test to verify the homoskedasticity (homogeneity of variances).
			homogeneous <- var.test(level_1_values, level_2_values)$p.value > 0.05 # If we obtain a p-value greater than 0.05, then we fail to reject the null and can assume that the two variances are homogeneous
			different_means <- t.test(level_1_values, level_2_values, var.equal = homogeneous, paired = FALSE)$p.value <= 0.05 # if we obtain a p-value of <= 0.05 then we reject the null and conclude that the true difference in means is not equal to 0
			
			cat(paste('\n\nstatistically different means (check assumptions for t-test):', different_means, '\n\n'))

			# https://www.r-bloggers.com/wilcoxon-mann-whitney-rank-sum-test-or-test-u/
			# Comparison of the averages of two independent groups of samples, of which we can not assume a distribution of Gaussian type; is also known as Mann-Whitney U-test.
			cat('\n\nThe Wilcoxon-Matt-Whitney test (or Wilcoxon rank sum test, or Mann-Whitney U-test) is used when is asked to compare the means of two groups that do not follow a normal distribution: it is a non-parametrical test. (https://www.r-bloggers.com/wilcoxon-mann-whitney-rank-sum-test-or-test-u/)')

			different_means <- wilcox.test(level_1_values, level_2_values, correct = FALSE, paired = FALSE)$p.value <= 0.05
			cat(paste('\n\nstatistically different means (Wilcoxon-Matt-Whitney):', different_means, '\n\n'))
		}

	}else if(is.factor(dataset_column) || is.logical(dataset_column)){
		
		print(ggplot(data = classification_dataset, mapping = aes(x = dataset_column)) + 
			geom_bar(mapping=aes(fill = dataset_column)) + 
			ggtitle(paste('Histogram of', column_name)) + ylab('Count') + xlab(column_name) + theme(axis.text.x = element_text(angle = 50, hjust = 1)) + guides(fill = FALSE))

		df_data <- data.frame(categorical_variable=classification_dataset$target, column = classification_dataset[, column_name ])
		print(ggplot(df_data, aes(column, ..count..)) +
				geom_bar(aes(fill = categorical_variable), position = 'dodge') +
				ggtitle(paste('Histogram of', column_name, ', grouped by the target variable')) + ylab('Count') + xlab(column_name) + theme(axis.text.x = element_text(angle = 50, hjust = 1)) + scale_fill_manual(values=colors_two))

		result = tryCatch({
			count_table <- table(classification_dataset[, c('target', column_name)])
			cat(paste0('\n\n> Chi-Square p-value: `', round(chisq.test(count_table)$p.value, 3), '`\n\n'))
		}, error = function(e) {
			cat(paste('\n\n', e, '\n\n'))
		})

	}else{
		stop(paste('unknown data type -', column_name))
	}
})
```

# Spot-Check

```{r spot_check_prepare_classification, echo = FALSE}
set.seed(custom_seed)
training_index <- createDataPartition(classification_dataset$target, p = training_percentage, list = TRUE)
classification_train <- classification_dataset[training_index$Resample1, ]
classification_test <- classification_dataset[-training_index$Resample1, ]
```

## Class Balance 

Make sure class balance is even amount training/test datasets.

### Training Data

```{r echo = FALSE}
prop.table(table(classification_train$target))
```

### Test

```{r echo = FALSE}
prop.table(table(classification_test$target))
```

```{r spot_check_classification, echo = FALSE}
# we have to make sure they are fit on exactly the same test sets in training and cross validation so we are sure we are making an apples-to-apples comparison
set.seed(custom_seed)
spot_check_folds <- createMultiFolds(classification_train$target, k = cross_validation_num_folds, times = cross_validation_num_repeats) # maintains the `churn` ratio

train_control <- trainControl(
	summaryFunction = twoClassSummary,
	classProbs = TRUE,
	method = 'repeatedcv',
	number = cross_validation_num_folds,
	repeats = cross_validation_num_repeats,
	verboseIter = FALSE,
	savePredictions = TRUE,
	index = spot_check_folds)

metric <- 'ROC'
```

> Using ``r cross_validation_num_folds``-fold cross-validation with ``r cross_validation_num_repeats`` repeats, using the ``r metric`` statistic to evaluate each model.

> used ``r percent(training_percentage)`` of data for `training` set (``r nrow(classification_train)``), and ``r percent(1 - training_percentage)`` for `test` set (``r nrow(classification_test)``).

### Logistic Regression - no pre processing

> NOTE that for logistic regression (GLM), caret's `train()` (because of `glm()`) uses the second-level factor value as the success/postive event but `resamples()` uses the first-level as the success event. The result is either the `sensitivity` and `specificity` for `resamples()` will be reversed (and so I would be unable to compare apples to apples with other models), or I need to keep the first-level factor as the positive event (the default approach), which will mean that THE COEFFICIENTS WILL BE REVERSED, MAKIN THE MODEL RELATIVE TO THE NEGATIVE EVENT. I chose the latter, in order to compare models below, but this means that when using the logistic model to explain the data, the reader needs to mentally reverse the direction/sign of the coefficients, or correct the problem in the final stages of model building.

```{r classification_no_pre_processing}
if(refresh_models)
{
	check_data(classification_train, validate_n_p = TRUE)
	set.seed(custom_seed)
	#model_glm_no_pre_processing <- train(target ~ ., data=classification_train %>% mutate(target = factor(target, levels = c('no', 'yes'))), method='glm', metric=metric, trControl=train_control)
	model_glm_no_pre_processing <- train(target ~ ., data=classification_train, method='glm', metric=metric, trControl=train_control)
	saveRDS(model_glm_no_pre_processing, file = './classification_data/model_glm_no_pre_processing.RDS')
} else{
	model_glm_no_pre_processing <- readRDS('./classification_data/model_glm_no_pre_processing.RDS')
}
summary(model_glm_no_pre_processing)
```

> NOTE: "Logistic regression does not make many of the key assumptions of linear regression and general linear models that are based on ordinary least squares algorithms – particularly regarding linearity, normality, homoscedasticity, and measurement level." [link](http://www.statisticssolutions.com/assumptions-of-logistic-regression/)

### Logistic Regression - basic pre-processing

> NOTE that for logistic regression (GLM), caret's `train()` (because of `glm()`) uses the second-level factor value as the success/postive event but `resamples()` uses the first-level as the success event. The result is either the `sensitivity` and `specificity` for `resamples()` will be reversed (and so I would be unable to compare apples to apples with other models), or I need to keep the first-level factor as the positive event (the default approach), which will mean that THE COEFFICIENTS WILL BE REVERSED, MAKIN THE MODEL RELATIVE TO THE NEGATIVE EVENT. I chose the latter, in order to compare models below, but this means that when using the logistic model to explain the data, the reader needs to mentally reverse the direction/sign of the coefficients, or correct the problem in the final stages of model building.

```{r classification_basic_processing}
if(refresh_models)
{
	check_data(classification_train, validate_n_p = TRUE)
	set.seed(custom_seed)
	model_glm_basic_processing <- train(target ~ ., data=classification_train, method='glm', metric=metric, preProc=c('nzv', 'center', 'scale', 'knnImpute'), trControl=train_control)
	saveRDS(model_glm_basic_processing, file = './classification_data/model_glm_basic_processing.RDS')
} else{
	model_glm_basic_processing <- readRDS('./classification_data/model_glm_basic_processing.RDS')
}
summary(model_glm_basic_processing)
```

> NOTE: "Logistic regression does not make many of the key assumptions of linear regression and general linear models that are based on ordinary least squares algorithms – particularly regarding linearity, normality, homoscedasticity, and measurement level." [link](http://www.statisticssolutions.com/assumptions-of-logistic-regression/)

### Logistic Regression - remove collinear data - based on caret's recommendation

> NOTE that for logistic regression (GLM), caret's `train()` (because of `glm()`) uses the second-level factor value as the success/postive event but `resamples()` uses the first-level as the success event. The result is either the `sensitivity` and `specificity` for `resamples()` will be reversed (and so I would be unable to compare apples to apples with other models), or I need to keep the first-level factor as the positive event (the default approach), which will mean that THE COEFFICIENTS WILL BE REVERSED, MAKIN THE MODEL RELATIVE TO THE NEGATIVE EVENT. I chose the latter, in order to compare models below, but this means that when using the logistic model to explain the data, the reader needs to mentally reverse the direction/sign of the coefficients, or correct the problem in the final stages of model building.

```{r logistic_regression_collinear_caret}
if(refresh_models)
{
	check_data(classification_train[, recommended_columns_caret], validate_n_p = TRUE)
	set.seed(custom_seed)
	glm_remove_collinearity_caret <- train(target ~ ., data = classification_train[, recommended_columns_caret], method = 'glm', metric=metric, preProc=c('nzv', 'center', 'scale', 'knnImpute'), trControl = train_control)
	saveRDS(glm_remove_collinearity_caret, file = './classification_data/glm_remove_collinearity_caret.RDS')
} else{
	glm_remove_collinearity_caret <- readRDS('./classification_data/glm_remove_collinearity_caret.RDS')
}
summary(glm_remove_collinearity_caret)
```

> NOTE: "Logistic regression does not make many of the key assumptions of linear regression and general linear models that are based on ordinary least squares algorithms – particularly regarding linearity, normality, homoscedasticity, and measurement level." [link](http://www.statisticssolutions.com/assumptions-of-logistic-regression/)

### Logistic Regression - remove collinear data - based on calculation

> NOTE that for logistic regression (GLM), caret's `train()` (because of `glm()`) uses the second-level factor value as the success/postive event but `resamples()` uses the first-level as the success event. The result is either the `sensitivity` and `specificity` for `resamples()` will be reversed (and so I would be unable to compare apples to apples with other models), or I need to keep the first-level factor as the positive event (the default approach), which will mean that THE COEFFICIENTS WILL BE REVERSED, MAKIN THE MODEL RELATIVE TO THE NEGATIVE EVENT. I chose the latter, in order to compare models below, but this means that when using the logistic model to explain the data, the reader needs to mentally reverse the direction/sign of the coefficients, or correct the problem in the final stages of model building.

```{r logistic_regression_collinear_calc}
if(refresh_models)
{
	check_data(classification_train[, recommended_columns_custom], validate_n_p = TRUE)
	set.seed(custom_seed)
	glm_remove_collinearity_custom <- train(target ~ ., data = classification_train[, recommended_columns_custom], method = 'glm', metric=metric, preProc=c('nzv', 'center', 'scale', 'knnImpute'), trControl = train_control)
	saveRDS(glm_remove_collinearity_custom, file = './classification_data/glm_remove_collinearity_custom.RDS')
} else{
	glm_remove_collinearity_custom <- readRDS('./classification_data/glm_remove_collinearity_custom.RDS')
}
summary(glm_remove_collinearity_custom)
```

> NOTE: "Logistic regression does not make many of the key assumptions of linear regression and general linear models that are based on ordinary least squares algorithms – particularly regarding linearity, normality, homoscedasticity, and measurement level." [link](http://www.statisticssolutions.com/assumptions-of-logistic-regression/)

### Logistic Regression - remove collinear data - based on calculation

> NOTE that for logistic regression (GLM), caret's `train()` (because of `glm()`) uses the second-level factor value as the success/postive event but `resamples()` uses the first-level as the success event. The result is either the `sensitivity` and `specificity` for `resamples()` will be reversed (and so I would be unable to compare apples to apples with other models), or I need to keep the first-level factor as the positive event (the default approach), which will mean that THE COEFFICIENTS WILL BE REVERSED, MAKIN THE MODEL RELATIVE TO THE NEGATIVE EVENT. I chose the latter, in order to compare models below, but this means that when using the logistic model to explain the data, the reader needs to mentally reverse the direction/sign of the coefficients, or correct the problem in the final stages of model building.

```{r logistic_regression_stepwise_backward}
if(refresh_models)
{
	check_data(classification_train, validate_n_p = TRUE)
	set.seed(custom_seed)
	
	set.seed(custom_seed)
	pre_processed_numeric_data <- preProcess(classification_train, method = c('nzv', 'center', 'scale', 'knnImpute')) # ignores non-numeric data
	columns_not_in_preprocessed_data <- colnames(classification_train)[!(colnames(classification_train) %in% colnames(pre_processed_numeric_data$data))] # figure out which columns we need to add back in (i.e. all (non-numeric) that are in classification_train but are NOT in pre_processed_numeric_data
	pre_processed_classification_train <- cbind(classification_train[, columns_not_in_preprocessed_data], pre_processed_numeric_data$data)

	set.seed(custom_seed)
	stepwise_glm_model <- step(glm(family = binomial, formula = target ~ ., data = pre_processed_classification_train), direction="backward", trace=0) # do stepwise regression (glm doesn't like factor target variables), then use the formula in train in order to take advantage of k-fold Cross Validation
	# stepwise_glm_model$formula gives the `optimal` formula (need to do this because coefficients will have factor variable names, not original columns. The formula will exclude columns not statistically significant)
	# now feed this back into training to do cross validation
	logistic_regression_stepwise_backward <- train(stepwise_glm_model$formula, data = classification_train, method = 'glm', metric=metric, preProc=c('nzv', 'center', 'scale', 'knnImpute'), trControl = train_control)
	saveRDS(logistic_regression_stepwise_backward, file = './classification_data/logistic_regression_stepwise_backward.RDS')
} else{
	logistic_regression_stepwise_backward <- readRDS('./classification_data/logistic_regression_stepwise_backward.RDS')
}
summary(logistic_regression_stepwise_backward)
```

> NOTE: "Logistic regression does not make many of the key assumptions of linear regression and general linear models that are based on ordinary least squares algorithms – particularly regarding linearity, normality, homoscedasticity, and measurement level." [link](http://www.statisticssolutions.com/assumptions-of-logistic-regression/)

### Linear Discriminant Analysis

```{r linear_discriminant_analsysis}
if(refresh_models)
{
	check_data(classification_train, validate_n_p = TRUE)
	set.seed(custom_seed)
	linear_discriminant_analsysis <- train(target ~ ., data = classification_train, method = 'lda', metric=metric, preProc=c('nzv', 'center', 'scale'), trControl = train_control)
	saveRDS(linear_discriminant_analsysis, file = './classification_data/linear_discriminant_analsysis.RDS')
} else{
	linear_discriminant_analsysis <- readRDS('./classification_data/linear_discriminant_analsysis.RDS')
}
summary(linear_discriminant_analsysis)
varImp(linear_discriminant_analsysis, scale = FALSE)
```

### Linear Discriminant Analysis - Remove Collinear Data Based on Caret's Recommendation

```{r linear_discriminant_analsysis_remove_collinear}
if(refresh_models)
{
	check_data(classification_train[, recommended_columns_caret], validate_n_p = TRUE)
	set.seed(custom_seed)
	linear_discriminant_analsysis_remove_collinear <- train(target ~ ., data = classification_train[, recommended_columns_caret], method = 'lda', metric=metric, preProc=c('nzv', 'center', 'scale'), trControl = train_control)
	saveRDS(linear_discriminant_analsysis_remove_collinear, file = './classification_data/linear_discriminant_analsysis_remove_collinear.RDS')
} else{
	linear_discriminant_analsysis_remove_collinear <- readRDS('./classification_data/linear_discriminant_analsysis_remove_collinear.RDS')
}
summary(linear_discriminant_analsysis_remove_collinear)
varImp(linear_discriminant_analsysis_remove_collinear, scale = FALSE)
```

### Partial Least Squares Discriminant Analysis (PLSDA)

```{r partial_least_squares_discriminant_analysis}
if(refresh_models)
{
	check_data(classification_train, validate_n_p = TRUE)
	set.seed(custom_seed)
	partial_least_squares_discriminant_analysis <- train(target ~ ., data = classification_train, method = 'pls', metric=metric, preProc=c('nzv', 'center', 'scale'), trControl = train_control, # "performance of PLS is affected when including predictors that contain little or no predictive information" i.e. remove NZV
														tuneGrid = expand.grid(.ncomp = tuning_number_of_latent_variables_to_retain))
	saveRDS(partial_least_squares_discriminant_analysis, file = './classification_data/partial_least_squares_discriminant_analysis.RDS')
} else{
	partial_least_squares_discriminant_analysis <- readRDS('./classification_data/partial_least_squares_discriminant_analysis.RDS')
}
partial_least_squares_discriminant_analysis
#summary(partial_least_squares_discriminant_analysis)
plot(partial_least_squares_discriminant_analysis, top = 20, scales = list(y = list(cex = 0.95)))
varImp(partial_least_squares_discriminant_analysis, scale = FALSE)
```

### glmnet (LASSO and RIDGE)

```{r glmnet_lasso_ridge}
if(refresh_models)
{
	check_data(classification_train, validate_n_p = TRUE)
	set.seed(custom_seed)
	glmnet_lasso_ridge <- train(target ~ ., data = classification_train, method = 'glmnet', metric=metric, preProc=c('nzv', 'center', 'scale'), trControl = train_control,
														tuneGrid = expand.grid(alpha = tuning_glmnet_alpha, lambda = tuning_glmnet_lambda))
	saveRDS(glmnet_lasso_ridge, file = './classification_data/glmnet_lasso_ridge.RDS')
} else{
	glmnet_lasso_ridge <- readRDS('./classification_data/glmnet_lasso_ridge.RDS')
}
#glmnet_lasso_ridge
summary(glmnet_lasso_ridge)
plot(glmnet_lasso_ridge, top = 20, scales = list(y = list(cex = 0.95)))
plot(glmnet_lasso_ridge, plotType = 'level')
varImp(glmnet_lasso_ridge, scale = FALSE)
```

### Sparse LDA

```{r sparse_lda}
if(refresh_models)
{
	check_data(classification_train, validate_n_p = FALSE)
	set.seed(custom_seed)
	sparse_lda <- train(target ~ ., data = classification_train, method = 'sparseLDA', metric=metric, preProc=c('nzv', 'center', 'scale'), trControl = train_control,
														tuneLength = 5)
	saveRDS(sparse_lda, file = './classification_data/sparse_lda.RDS')
} else{
	sparse_lda <- readRDS('./classification_data/sparse_lda.RDS')
}
#sparse_lda
summary(sparse_lda)
plot(sparse_lda, top = 20, scales = list(y = list(cex = 0.95)))
plot(sparse_lda, plotType = 'level')
varImp(sparse_lda, scale = FALSE)
```

### Nearest Shrunken Centroids

> Poor performance and varImp() gives error

```{r nearest_shrunken_centroids, eval=FALSE, include=FALSE}
if(refresh_models)
{
	check_data(classification_train, validate_n_p = FALSE)
	set.seed(custom_seed)
	nearest_shrunken_centroids <- train(target ~ ., data = classification_train, method = 'pam', metric=metric, preProc=c('nzv', 'center', 'scale'), trControl = train_control,
														tuneGrid = tuning_nearest_shrunken_centroids_shrinkage_threshold)

	saveRDS(nearest_shrunken_centroids, file = './classification_data/nearest_shrunken_centroids.RDS')
} else{
	nearest_shrunken_centroids <- readRDS('./classification_data/nearest_shrunken_centroids.RDS')
}
#nearest_shrunken_centroids
summary(nearest_shrunken_centroids)
predictors(nearest_shrunken_centroids)
plot(nearest_shrunken_centroids, top = 20, scales = list(y = list(cex = 0.95)))
varImp(nearest_shrunken_centroids, scale = FALSE)
```

### Random Forest


### Neural Network


### Ada Boost

### All Models on Page 550 that are classification or both regression and classification

### Models used for spot-check.Rmd 


## Resamples & Top Models

### Resamples

```{r resamples_regression, echo=FALSE, fig.height=6, fig.width=9}
classification_models <- list(
		model_glm_no_pre_processing = model_glm_no_pre_processing,
		model_glm_basic_processing = model_glm_basic_processing,
		glm_remove_collinearity_caret = glm_remove_collinearity_caret,
		glm_remove_collinearity_custom = glm_remove_collinearity_custom,
		logistic_regression_stepwise_backward = logistic_regression_stepwise_backward,
		linear_discriminant_analsysis = linear_discriminant_analsysis,
		linear_discriminant_analsysis_remove_collinear = linear_discriminant_analsysis_remove_collinear,
		partial_least_squares_discriminant_analysis = partial_least_squares_discriminant_analysis,
		glmnet_lasso_ridge = glmnet_lasso_ridge,
		sparse_lda = sparse_lda#,
		#nearest_shrunken_centroids = nearest_shrunken_centroids
		)

resamples <- resamples(classification_models)

# Table comparison
(resamples_summary <- summary(resamples))

resamples_long <- resamples$values %>%
	dplyr::select(-Resample) %>%
	gather(type, value) %>%
	separate(col=type, into=c('model', 'metric'), sep = '~') %>%
	mutate(model = factor(model, levels = rev(resamples$models)), metric = factor(metric, levels = c('ROC', 'Sens', 'Spec')))

resamples_means <- resamples_long %>%
	dplyr::group_by(model, metric) %>%
	dplyr::summarise(average = mean(value)) %>%
	dplyr::ungroup()

max_roc_mean <- max(resamples_means %>% filter(metric == 'ROC') %>% dplyr::select(average))
max_sensitivity_mean <- max(resamples_means %>% filter(metric == 'Sens') %>% dplyr::select(average))
max_specificity_mean <- max(resamples_means %>% filter(metric == 'Spec') %>% dplyr::select(average))

best_values <- data.frame(metric = levels(resamples_long$metric), Z = c(max_roc_mean, max_sensitivity_mean, max_specificity_mean))
ggplot(data = resamples_long, mapping = aes(x = model, y = value)) +
	geom_boxplot() + 
	stat_summary(fun.y=mean, colour="red", geom="point", shape=21, size=3, show.legend = FALSE) + 
	geom_hline(data = best_values, aes(yintercept = Z), color='red') +
	facet_grid(. ~ metric, scales = "free", space = "free_y") +
	coord_flip()

resamples$values %>% dplyr::select(contains('~ROC')) %>%
	gather(model, ROC) %>%
	mutate(model = factor(model)) %>%
ggplot() + 
	geom_density(aes(ROC, color = model)) + 
    geom_jitter(aes(ROC, 0, color = model), alpha = 0.5, height = 0.02) 

# densityplot(resamples, metric = 'ROC')
densityplot(resamples, metric = 'Sens')
densityplot(resamples, metric = 'Spec')
```

# Train Top Models on Entire Training Dataset & Predict on Test Set

> after using cross-validation to tune, we'll take the highest ranked models, retrain the models (with the final tuning parameters) on the entire training set, and predict using the test set.

```{r model_definitions, echo=FALSE, fig.height=5, fig.width=8}
model_plots <- function(model, testing_set, prediction_threshold = 0.5){

	predictions_probabilities_raw <- predict(model, testing_set %>% dplyr::select(-target), type = 'prob')
	actual_observations <- testing_set$target
	predicted_probabilities_negative <- predictions_probabilities_raw[, target_negative_class]
	predicted_probabilities_positive <- predictions_probabilities_raw[, target_positive_class]

	names(predicted_probabilities_negative) <- NULL
	names(predicted_probabilities_positive) <- NULL

	df_results <- data.frame(	actual_observations = actual_observations,
						 		predicted_probabilities = predicted_probabilities_positive,
						 		predicted_probabilities_negative = predicted_probabilities_negative)
	df_results$predicted_class <- predict(model, testing_set %>% dplyr::select(-target))
	df_results$label <- ifelse(actual_observations == target_negative_class,
									paste('True Outcome:', target_negative_class), paste('True Outcome:', target_positive_class))

	# prop.table(table(classification_test$target))[target_positive_class]
	# ggplot(df_results, aes(predicted_probabilities)) + geom_histogram(binwidth = 0.1)

	### Plot the probability of negative/positive
	print(histogram(~predicted_probabilities_negative | label, data = df_results, layout = c(2, 1), nint = 20, xlab = paste0('Probability of `', target_negative_class,'`'), type = 'count'))
	print(histogram(~predicted_probabilities | label, data = df_results, layout = c(2, 1), nint = 20, xlab = paste0('Probability of `', target_positive_class,'`'), type = 'count'))

	### Create the confusion matrix from the test set.
	confusion_matrix <- table(actual = actual_observations, predictions = df_results$predicted_class)
	confusion_ls <- confusion_list(true_pos = confusion_matrix[target_positive_class, target_positive_class],
									true_neg = confusion_matrix[target_negative_class, target_negative_class],
									false_pos = confusion_matrix[target_negative_class, target_positive_class],
									false_neg = confusion_matrix[target_positive_class, target_negative_class])
	observation_rates <- confusion_matrix / sum(confusion_matrix)

	average_profit <- 5
	average_cost <- 2
	gain_true_positive <- average_profit - average_cost # positive is gain
	gain_true_negative <- 0
	gain_false_positive <- -average_cost # negative is cost
	gain_false_negative <- -(average_profit - average_cost) # more than likely, opportunity cost, but still should be counted (opportunity cost of not making the profit, but had we labed as a positive, we would have also incurred the average_cost (e.g. of mailing the letter, etc.))
	gain_cost_matrix <- matrix(c(gain_true_negative, gain_false_positive, gain_false_negative, gain_true_positive), nrow = 2, byrow = TRUE, dimnames = list(c('Actual Negative', 'Actual Positive'), c('Predicted Negative', 'Predicted Positive')))

	print(expected_value_chart(predicted_probabilities_positive = predicted_probabilities_positive, actual_outcomes = actual_observations == target_positive_class, gain_cost_matrix = gain_cost_matrix))

	print(visualize_quality_of_model(confusion_ls))
	conf_matrix <- confusionMatrix(data = df_results$predicted_class, reference = df_results$actual_observations)
	cat(codebc(conf_matrix))

	### ROC curves:
	roc_pred <- prediction(predictions = predicted_probabilities_positive, labels = actual_observations)
	roc_perf <- performance(roc_pred, measure = 'sens', x.measure = 'fpr')
	print(plot(roc_perf, colorize = TRUE, print.cutoffs.at = seq(0,1,0.05), text.adj = c(-0.2, 1.7)))

	# calculate AUC
	perf_auc <- performance(roc_pred, measure = 'auc')
	model_auc <- unlist(perf_auc@y.values)

	# Gain/Lift charts
	creditLift <- lift(actual_observations ~ predicted_probabilities_positive, data = df_results)
	print(xyplot(creditLift))

	gl_table <- gain_lift_table(actual_observations = actual_observations, predicted_probabilities = predicted_probabilities_positive, target_positive_class = target_positive_class)
	gl_charts <- gain_lift_charts(gl_table = gl_table)

	print(gl_charts[[1]])
	print(gl_charts[[2]])

	# Calibration Chart
	calibration_data <- calibration(actual_observations ~ predicted_probabilities, data = df_results, cuts = 10)
	print(xyplot(calibration_data, auto.key = list(columns = 2)))

	cal_table <- calibration_table(actual_observations = actual_observations, predicted_probabilities = predicted_probabilities_positive, target_positive_class = target_positive_class)
	cal_chart <- calibration_chart(cal_table = cal_table)
	print(cal_chart)

	return (c(model_auc = model_auc, model_sensitivity = as.numeric(conf_matrix$byClass['Sensitivity']), model_specificity = as.numeric(conf_matrix$byClass['Specificity'])))
}
```

```{r top_models, echo=FALSE, fig.height=5, fig.width=8}
top_models <- resamples_means %>% 
	filter(metric == 'ROC') %>% 
	mutate(rank = row_number(average)) %>% 
	filter(rank <= 5) %>% # LOWEST RMSE
	arrange(rank) %>% 
	dplyr::select(model, average) %>%
	dplyr::rename(cross_validation_auc = average) %>%
	mutate(model_object = map(model, ~ { classification_models[names(classification_models) == .][[1]] })) # add models to data.frame so when we choose the top models later we will 

final_train_control <- trainControl(method = "none", summaryFunction = twoClassSummary, classProbs = TRUE, savePredictions = 'all') # In cases where the model tuning values are known, train can be used to fit the model to the entire training set without any resampling or parameter tuning. Using the method = "none" option in trainControl can be used https://topepo.github.io/caret/model-training-and-tuning.html#fitting-models-without-parameter-tuning

model_stats <- map2(as.character(top_models$model), top_models$model_object, ~ {
	model_name <- .x
	model_object <- .y

	cat(paste0('\n\n### ', model_object$modelInfo$label, ' (', model_name,')\n\n'))
	
	# get method, tuning grid, pre-processing steps.
	model_method <- model_object$method
	tune_grid <- model_object$bestTune
	
	if(is.null(model_object$preProcess$method))
	{
		pre_process <- NULL
	}else{
		pre_process_call <- as.character(model_object$call$preProc)
		pre_process <- pre_process_call[!(pre_process_call %in% c('c'))]
		
		cat(paste0('\n\nPre-Processing: `', paste(pre_process, collapse = ', '), '`\n\n'))
	}
	
	if(refresh_models)
	{
		set.seed(custom_seed)
		# some models have different parameters for `train`, which we need to use when training the final model
		if(model_method == 'asdf')
		{
			#final_model <- train(
			#	target ~ ., data = classification_train, method = model_method, trControl = final_train_control, preProc = pre_process, tuneGrid = tune_grid, 
			#	ntree = parameter_random_forest_ntree, importance = TRUE)

			set.seed(custom_seed)
  			final_model <- randomForest(target ~ ., data = classification_train, ntree = 2000)

		}else{
			set.seed(custom_seed)
			final_model <- train(target ~ ., data = classification_train, method = model_method, metric = metric, preProc = pre_process, trControl = final_train_control, tuneGrid = tune_grid)
		}

		saveRDS(final_model, file = paste0('./classification_data/top_model_', model_name, '.RDS'))

	}else{

		final_model <- readRDS(file = paste0('./classification_data/top_model_', model_name, '.RDS'))
	}

	print(summary(final_model$finalModel))
	model_stats <- model_plots(model = final_model, testing_set = classification_test)
	return (model_stats)
})

final_models_metrics <- data.frame(model = factor(as.character(top_models$model), levels = as.character(top_models$model)),
								   test_set_auc = map_dbl(model_stats, ~ .['model_auc']),
								   cross_validation_auc = top_models$cross_validation_auc,
								   sensitivity = map_dbl(model_stats, ~ .['model_sensitivity']),
								   specificity = map_dbl(model_stats, ~ .['model_specificity']))

max_test_auc <- max(final_models_metrics$test_set_auc)
max_sensitivity <- max(final_models_metrics$sensitivity)
max_specificity <- max(final_models_metrics$specificity)

#prepare/format data for ggplot
final_models_metrics_long <- gather(final_models_metrics, metric, value, -model) %>%
	mutate(model = factor(model, levels = rev(unique(model))),
		type = ifelse(str_detect(metric, 'auc'), str_split(metric, '_'), '')) %>%
	mutate(type = map_chr(type, ~ {
			if(length(.) > 1)
			{
				return(paste0(.[1:2], collapse = ' '))
			}else{
				return ('test set')
			}
		}),
	metric = ifelse(str_detect(metric, 'auc'), 'auc', as.character(metric))) %>%
	mutate(metric = factor(metric, levels = unique(metric)))

best_values <- data.frame(metric = unique(final_models_metrics_long$metric), Z = c(max_test_auc, max_sensitivity, max_specificity))
ggplot(data = final_models_metrics_long, mapping = aes(x = model, y = value, col = type)) +
	geom_point(size = 3, alpha = 0.7) +
	geom_hline(data = best_values, aes(yintercept = Z), color='red') +
	facet_grid(. ~ metric, scales = "free", space = "free_y") +
	coord_flip()
```
