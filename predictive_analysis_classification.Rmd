---
title: "Predictive Analysis"
author: "Shane Kercheval"
output:
  md_document:
    variant: markdown_github
    toc: true
    toc_depth: 3
---

```{r setup, echo = FALSE, results='hide', message=FALSE, warning=FALSE}
options(scipen=999) # non-scientific notation
options(width=180)
source('~/r-tools/output/plots.R', chdir=TRUE)
source('~/r-tools/output/markdown.R', chdir=TRUE)
source('~/r-tools/general/model_measurements.R', chdir=TRUE)
library('data.table')
library(partykit) # Use the partykit package to make some nice plots. First, convert the rpart objects to party objects.
library(psych)
library(knitr)
library(scales)
library(DMwR)
library(e1071)
library(Hmisc)
library(corrplot)
library(stringr)
library(tidyverse)
library(caret)
library(gmodels)
library(randomForest)
library(ROCR)

opts_chunk$set(out.width='750px', dpi=200)

require(doMC)
registerDoMC(cores = round(detectCores()/2) + 1) # All subsequent models are then run in parallel - https://topepo.github.io/caret/parallel-processing.html

seed <- 123
refresh_models <- FALSE
```

```{r dataset_definition, echo = FALSE}
credit <- fread('./classification_data/credit.csv')
classification_dataset <- credit %>%
	dplyr::rename(target = default) %>%
	dplyr::mutate(phone = ifelse(phone == 'yes', TRUE, FALSE)) %>% # technically don't need to do this but there seems to be an error in dplyr.mutate_if https://github.com/tidyverse/dplyr/pull/2011 which says it is resolved, but still seems to be a problem, dispite having latest version
	dplyr::select(target, everything())
classification_dataset <- as.data.frame(classification_dataset)

target_positive_class <- 'yes'
target_negative_class <- 'no'
```

```{r dataset_prepare, echo = FALSE}
classification_dataset$target <- factor(classification_dataset$target, levels = c(target_positive_class, target_negative_class))

classification_dataset <- classification_dataset %>%
	dplyr::mutate_if(is.character, as.factor) %>%
	dplyr::mutate_if(is.logical, as.factor)

classification_column_names <- colnames(classification_dataset)
numeric_columns <- map_lgl(classification_column_names, ~ {
	return (is.numeric(classification_dataset[, .]))
})
numeric_column_names <- classification_column_names[numeric_columns]

sample_size <- nrow(classification_dataset)
num_predictors <- length(classification_dataset) - 1
```

# Tuning Parameters

```{r tuning_parameters}
# train/test set
training_percentage <- 0.90

# cross validation
cross_validation_num_folds <- 10
cross_validation_num_repeats <- 3
```

# Dataset

> Assumes the dataset has factors for strings; logical for TRUE/FALSE; `target` for outcome variable

## Summary

> Total predictors: ``r num_predictors``

> Total data-points/rows: ``r sample_size``

> Number of training data-points: ``r round(sample_size * training_percentage)``

Rule of thumbs for dimensions (Probabilistic and Statistical Modeling in Computer Science; pg 430):

> r < sqrt(n); where r is the number of predictors and sqrt(n) is the square root of the sample size (``r round(sqrt(sample_size))``): ``r num_predictors < round(sqrt(sample_size))``

> r < sqrt(n_t); where r is the number of predictors and sqrt(n_t) is the square root of the training set size (``r round(sqrt(round(sample_size * training_percentage)))``): ``r num_predictors < round(sqrt(round(sample_size * training_percentage)))``

```{r summary, echo = FALSE}
summary(classification_dataset)
```

## Skewness

Note: `Box-Cox` can only be applied to sets (i.e. predictors) where all values are `> 0`. So some/most/all? `NA`s will be from that limiation.

```{r skewness, echo = FALSE}
skewnewss_statistics <- map_dbl(classification_column_names, ~ {
	column_name <- .

	dataset_column <- classification_dataset[, column_name]
	if(is.numeric(dataset_column) && min(dataset_column > 0))
	{
		return (BoxCoxTrans(dataset_column)$skewness)
	}else{
		return (NA)
	}
})

kable(data.frame(column = classification_column_names, boxcox_skewness = skewnewss_statistics))
```

## Outliers

```{r outliers, echo = FALSE}
outliers <- map(numeric_column_names, ~ {
	numeric_data <- classification_dataset[, .]

	outlier_range <- 2 # this multipied by IQR (traditionally this number is 1.5)

	lower_quantile <- quantile(numeric_data)[2]
	upper_quantile <- quantile(numeric_data)[4]
	iqr <- upper_quantile - lower_quantile

	threshold_upper <- (iqr * outlier_range) + upper_quantile
	threshold_lower <- lower_quantile - (iqr * 1.5) # Any numeric_data point outside (> threshold_upper or < threshold_lower) these values is a mild outlier

	upper_outlier_count <- sum(numeric_data > threshold_upper)
	lower_outlier_count <- sum(numeric_data < threshold_lower)

	return (list(upper_outlier_count, lower_outlier_count))
})

outliers <- data.frame(columns = numeric_column_names, lower_outlier_count = map_chr(outliers, ~ {.[[2]]}), upper_outlier_count = map_chr(outliers, ~ {.[[1]]}))
kable(outliers)
```

## Correlation & Collinearity

```{r correlation, echo = FALSE, fig.height=10, fig.width=10}
#pairs.panels(classification_dataset[, numeric_columns])
correlations <- cor(classification_dataset[, numeric_columns])
corrplot::corrplot(correlations, order = 'hclust', tl.cex = .35, col = colorRampPalette(rev(c('#67001F', '#B2182B', '#D6604D', '#F4A582', '#FDDBC7', '#FFFFFF', '#D1E5F0', '#92C5DE', '#4393C3', '#2166AC', '#053061')))(200))
```

### Collinearity Removal

```{r removal_setup, echo = FALSE}
correlation_threshold <- 0.90
```

#### Caret's `findCorrelation`

Shows caret's recommendation of removing collinear columns based on correlation threshold of ``r correlation_threshold``

```{r caret_collinearity, echo = FALSE}
numeric_predictor_data <- classification_dataset[, numeric_columns]
collinear_indexes <- findCorrelation(cor(numeric_predictor_data), correlation_threshold)
if(length(collinear_indexes) == 0)
{
	recommended_columns_caret <- colnames(classification_dataset)
}else{
	recommended_columns_caret <- c('target', colnames(numeric_predictor_data)[-collinear_indexes])	
}
```

> columns recommended for removal: ``r paste(colnames(numeric_predictor_data)[collinear_indexes], collapse = ', ')``

> final columns recommended: ``r paste(recommended_columns_caret, collapse = ', ')``

#### Heuristic

This method is described in APM pg 47 as the following steps

- calculate the correlation matrix of predictors
- determine the two predictors associated with the largest absolute pairwise correlation (call them predictors `A` and `B`)
- Determine the average correlation between `A` and the other variables.
	- Do the same for `B`
- If `A` has a larger average correlation, remove it; otherwise, remove predcitor `B`
- Repeat until no absolute correlations are above the threshold (``r correlation_threshold``)

```{r heuristic_collinearity, echo = FALSE}
predictor_correlations <- cor(numeric_predictor_data)

walk(1:nrow(predictor_correlations), ~ {
	predictor_correlations[.,.] <<- NA	
})

while(max(abs(predictor_correlations), na.rm = TRUE) > correlation_threshold) {
	## caret's findCorrelation function is used to identify columns to remove.
	num_cols_rows <- nrow(predictor_correlations)
	#highCorr <- findCorrelation(predictor_correlations, .75)
	ab_index <- which(abs(predictor_correlations) == max(abs(predictor_correlations), na.rm = TRUE) & abs(predictor_correlations) > 0.75)[1] # get the first index of the highest correlation
	a_index <- ceil(ab_index / num_cols_rows) # get the row number 
	b_index <- ab_index - (num_cols_rows * (a_index-1))

	ave_a_predictor_correlations <- mean(abs(predictor_correlations[a_index, ]), na.rm = TRUE)
	ave_b_predictor_correlations <- mean(abs(predictor_correlations[b_index, ]), na.rm = TRUE)

	if(ave_a_predictor_correlations > ave_b_predictor_correlations)
	{
		predictor_correlations <<- predictor_correlations[-a_index, -a_index]
				
	}else
	{
		predictor_correlations <<- predictor_correlations[-b_index, -b_index]
	}	
	
}
recommended_numeric_columns <- colnames(predictor_correlations)
non_numeric_columns <- classification_column_names[!(classification_column_names %in% numeric_column_names)] # includes target
recommended_columns_custom <- c(recommended_numeric_columns, non_numeric_columns)
recommended_removal <- classification_column_names[!(classification_column_names %in% recommended_columns_custom)]
```

> columns recommended for removal: ``r paste(recommended_removal, collapse = ', ')``

> final columns recommended: ``r paste(recommended_columns_custom, collapse = ', ')``

## Graphs

```{r graphs, echo = FALSE, fig.height=4, fig.width=6, comment='', results='asis'}
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

colors_two = rev(gg_color_hue(2))

walk(classification_column_names, ~ {
	column_name <- .
	
	if(column_name == 'target')
	{
		return ()
	}

	cat(paste('\n\n###', column_name, '\n\n'))
	dataset_column <- classification_dataset[, column_name]
	if(is.numeric(dataset_column))
	{
		df_data <- data.frame(target=dataset_column)
		fun_args <- list(mean = mean(df_data$target), sd = sd(df_data$target))
		quantiles <- quantile(df_data$target, c(0.02, 0.98)) # cut off any 'outliers' (crude) so it doesn't skew graph
		
		result = tryCatch({
			print(ggplot(data = df_data, mapping = aes(x = target)) +
				geom_histogram(mapping=aes(y = ..density..), bins = 30) + # histogram of data
				geom_density(col = 'red') + # emperical PDF
				stat_function(fun = dnorm, args = fun_args, col = 'blue') + # theoretical normal PDF, based on data's mean & standard deviation
				coord_cartesian(xlim = c(quantiles[1], quantiles[2])) + labs(x = column_name))

		}, error = function(e) {
			cat('\n\nError with histogram.\n\n')
		})

		print(ggplot(data = df_data, mapping = aes(x = column_name,  y = target, group = 1)) +
				geom_boxplot() +
				xlab(column_name) + ylab(column_name))

		print(ggplot(data = classification_dataset, mapping = aes(x = target, y = dataset_column)) +
				geom_boxplot() + 
				xlab('target') + ylab(column_name))
		
		if(length(levels(classification_dataset$target)) == 2){
			levels <- levels(classification_dataset$target)
			level_1 <- levels[1]
			level_2 <- levels[2]

			level_1_values <- (classification_dataset %>% filter(target == level_1))[, column_name]
			level_2_values <- (classification_dataset %>% filter(target == level_2))[, column_name]

			#https://www.r-bloggers.com/two-sample-students-t-test-1/
			# Before proceeding with the t-test, it is necessary to evaluate the sample variances of the two groups, using a Fisherâ€™s F-test to verify the homoskedasticity (homogeneity of variances).
			homogeneous <- var.test(level_1_values, level_2_values)$p.value > 0.05 # If we obtain a p-value greater than 0.05, then we fail to reject the null and can assume that the two variances are homogeneous
			different_means <- t.test(level_1_values, level_2_values, var.equal = homogeneous, paired = FALSE)$p.value <= 0.05 # if we obtain a p-value of <= 0.05 then we reject the null and conclude that the true difference in means is not equal to 0
			
			cat(paste('\n\nstatistically different means (check assumptions for t-test):', different_means, '\n\n'))

			# https://www.r-bloggers.com/wilcoxon-mann-whitney-rank-sum-test-or-test-u/
			# Comparison of the averages of two independent groups of samples, of which we can not assume a distribution of Gaussian type; is also known as Mann-Whitney U-test.
			cat('\n\nThe Wilcoxon-Matt-Whitney test (or Wilcoxon rank sum test, or Mann-Whitney U-test) is used when is asked to compare the means of two groups that do not follow a normal distribution: it is a non-parametrical test. (https://www.r-bloggers.com/wilcoxon-mann-whitney-rank-sum-test-or-test-u/)')

			different_means <- wilcox.test(level_1_values, level_2_values, correct = FALSE, paired = FALSE)$p.value <= 0.05
			cat(paste('\n\nstatistically different means (Wilcoxon-Matt-Whitney):', different_means, '\n\n'))
		}

	}else if(is.factor(dataset_column) || is.logical(dataset_column)){
		
		print(ggplot(data = classification_dataset, mapping = aes(x = dataset_column)) + 
			geom_bar(mapping=aes(fill = dataset_column)) + 
			ggtitle(paste('Histogram of', column_name)) + ylab('Count') + xlab(column_name) + theme(axis.text.x = element_text(angle = 50, hjust = 1)) + guides(fill = FALSE))

		df_data <- data.frame(categorical_variable=classification_dataset$target, column = classification_dataset[, column_name ])
		print(ggplot(df_data, aes(column, ..count..)) +
				geom_bar(aes(fill = categorical_variable), position = 'dodge') +
				ggtitle(paste('Histogram of', column_name, ', grouped by the target variable')) + ylab('Count') + xlab(column_name) + theme(axis.text.x = element_text(angle = 50, hjust = 1)) + scale_fill_manual(values=colors_two))

		result = tryCatch({
			count_table <- table(classification_dataset[, c('target', column_name)])
			cat(paste0('\n\n> Chi-Square p-value: `', round(chisq.test(count_table)$p.value, 3), '`\n\n'))
		}, error = function(e) {
			cat(paste('\n\n', e, '\n\n'))
		})

	}else{
		stop(paste('unknown data type -', column_name))
	}
})
```

# Spot-Check

```{r spot_check_prepare_classification, echo = FALSE}
set.seed(seed)
training_index <- createDataPartition(classification_dataset$target, p = training_percentage, list = TRUE)
classification_train <- classification_dataset[training_index$Resample1, ]
classification_test <- classification_dataset[-training_index$Resample1, ]

prop.table(table(classification_train$target))
prop.table(table(classification_test$target))
```

```{r spot_check_classification, echo = FALSE}
# we have to make sure they are fit on exactly the same test sets in training and cross validation so we are sure we are making an apples-to-apples comparison
set.seed(seed)
spot_check_folds <- createMultiFolds(classification_train$target, k = cross_validation_num_folds, times = cross_validation_num_repeats) # maintains the `churn` ratio

train_control <- trainControl(
	summaryFunction = twoClassSummary,
	classProbs = TRUE,
	method = 'repeatedcv',
	number = cross_validation_num_folds,
	repeats = cross_validation_num_repeats,
	verboseIter = FALSE,
	savePredictions = TRUE,
	index = spot_check_folds)

metric <- 'ROC'
```

- Note: e.g. if there are rare values at the target extremes (lows/highs), the train and especially the test set might not be training/testing on them. Is the test set representative? If the test set doesn't have as extreme values, it can even predict better (e.g. lower RMSE higher Rsquared) than the average Cross Validation given on training because it's not using those extreme values.

> used ``r percent(training_percentage)`` of data for `training` set (``r nrow(classification_train)``), and ``r percent(1 - training_percentage)`` for `test` set (``r nrow(classification_test)``).

### Logistic Regression - no pre processing

```{r linear_regression_no_pre_processing}
if(refresh_models)
{
	set.seed(seed)
	glm_no_pre_processing <- train(target ~ ., data=classification_train, method='glm', metric=metric, trControl=train_control)
	saveRDS(glm_no_pre_processing, file = './classification_data/glm_no_pre_processing.RDS')
} else{
	glm_no_pre_processing <- readRDS('./classification_data/glm_no_pre_processing.RDS')
}
summary(glm_no_pre_processing)
#plot(glm_no_pre_processing$finalModel)
```

### Logistic Regression - basic processing

```{r linear_regression_basic_processing}
if(refresh_models)
{
	set.seed(seed)
	glm_basic_processing <- train(target ~ ., data=classification_train, method='glm', metric=metric, preProc=c('knnImpute', 'nzv', 'center', 'scale'), trControl=train_control)
	saveRDS(glm_basic_processing, file = './classification_data/glm_basic_processing.RDS')
} else{
	glm_basic_processing <- readRDS('./classification_data/glm_basic_processing.RDS')
}
summary(glm_basic_processing)
plot(glm_basic_processing$finalModel)
```


```{r, echo=FALSE, fig.height=6, fig.width=9}
set.seed(123)
final_model <- randomForest(target ~ ., data = classification_train, ntree = 2000)
#library(C50)
#set.seed(123)
#final_model <- C5.0(classification_train %>% select(-target), classification_train$target) # exclude 'default' column, but include it as target factor vector for classification
summary(final_model)
probabilities_raw <- predict(final_model, classification_test %>% dplyr::select(-target), type = 'prob')
actual_observations <- classification_test$target
probabilities_negative <- probabilities_raw[, target_negative_class]
probabilities_positive <- probabilities_raw[, target_positive_class]

names(probabilities_negative) <- NULL
names(probabilities_positive) <- NULL

df_results <- data.frame(actual_observations = actual_observations, predicted_probabilities = probabilities_positive, probabilities_negative = probabilities_negative)
df_results$predicted_class <- predict(final_model, classification_test %>% dplyr::select(-target))

df_results$label <- ifelse(actual_observations == target_negative_class, paste('True Outcome:', target_negative_class), paste('True Outcome:', target_positive_class))

### Plot the probability of bad credit
histogram(~probabilities_negative | label, data = df_results, layout = c(2, 1), nint = 20, xlab = paste0('Probability of `', target_negative_class,'`'), type = 'count')
histogram(~predicted_probabilities | label, data = df_results, layout = c(2, 1), nint = 20, xlab = paste0('Probability of `', target_positive_class,'`'), type = 'count')

### Create the confusion matrix from the test set.
confusion_matrix <- table(actual = actual_observations, predictions = df_results$predicted_class)
confusion_ls <- confusion_list(true_pos = confusion_matrix['yes', 'yes'],
								true_neg = confusion_matrix['no', 'no'],
								false_pos = confusion_matrix['no', 'yes'],
								false_neg = confusion_matrix['yes', 'no'])
observation_rates <- confusion_matrix / sum(confusion_matrix)

average_profit <- 28.40
average_cost <- 2
gain_true_positive <- average_profit - average_cost # positive is gain
gain_true_negative <- 0
gain_false_positive <- -average_cost # negative is cost
gain_false_negative <- -(average_profit - average_cost) # more than likely, opportunity cost, but still should be counted (opportunity cost of not making the profit, but had we labed as a positive, we would have also incurred the average_cost (e.g. of mailing the letter, etc.))
costs_gains <- matrix(c(gain_true_positive, gain_false_negative, gain_false_positive, gain_true_negative), nrow = 2, byrow = TRUE)

confusion_matrix
costs_gains
expected_value = sum(confusion_matrix * costs_gains)
expected_value

visualize_quality_of_model(confusion_ls)
confusionMatrix(data = df_results$predicted_class, reference = df_results$actual_observations)

### ROC curves:
roc_pred <- prediction(predictions = probabilities_positive, labels = actual_observations)
roc_perf <- performance(roc_pred, measure = 'sens', x.measure = 'fpr')
plot(roc_perf, colorize = TRUE, print.cutoffs.at = seq(0,1,0.05), text.adj = c(-0.2, 1.7))

# calculate AUC
perf_auc <- performance(roc_pred, measure = 'auc')
model_auc <- unlist(perf_auc@y.values)
model_auc

# Gain/Lift charts
creditLift <- lift(actual_observations ~ probabilities_positive, data = df_results)
xyplot(creditLift)

gl_table <- gain_lift_table(actual_observations = actual_observations, predicted_probabilities = probabilities_positive, target_positive_class = target_positive_class)
gl_charts <- gain_lift_charts(gl_table = gl_table)

gl_charts[[1]]
gl_charts[[2]]

# Calibration Chart
calibration_data <- calibration(actual_observations ~ predicted_probabilities, data = df_results, cuts = 10)
xyplot(calibration_data, auto.key = list(columns = 2))

cal_table <- calibration_table(actual_observations = actual_observations, predicted_probabilities = probabilities_positive, target_positive_class = target_positive_class)
cal_chart <- calibration_chart(cal_table = cal_table)

# recalibrate probabilities using Naive Bayes
require(klaR)
recalibrated_model <- NaiveBayes(actual_observations ~ predicted_probabilities, data = df_results, usekernel = TRUE)
recalibrated_probabilities_raw <- predict(recalibrated_model, df_results, type = 'prob')
recalibrated_probabilities <- recalibrated_probabilities_raw$posterior[, target_positive_class]
recal_table <- calibration_table(actual_observations = actual_observations, predicted_probabilities = recalibrated_probabilities, target_positive_class = target_positive_class)
recal_chart <- calibration_chart(cal_table = recal_table)

cal_chart
recal_chart
```
